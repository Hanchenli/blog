[{"content":"In this blog post, we will discuss NVidia\u0026rsquo;s recent announcements at CES and their acquisition of Groq, focusing on their strategy to enhance LLM agent inference. We will explore three main aspects: the importance of KV cache hits, the role of SRAM in improving decoding speed, and a proposed hardware-software architecture that potentially speeds up agent inference from task-based to real-time.\nPrerequisite: LLM inference basics. Suggested reading: LLM Inference; KV Cache Offloading with LMCache; LLM Agent with KV Cache\nCES Announcements and Groq Acquisition In CES 2026, NVIDIA introduced the Rubin architecture. As described by this announcement This new GPU architecture is designed to significantly boost the performance of large language model (LLM) inference by 10x. It incorporates NVIDIA Vera CPU, Rubin GPU, NVLink 6 Switch, ConnectX-9 SuperNIC, BlueField-4 DPU and Spectrum-6 Ethernet Switch. It natively supports the Inference Context Memory Storage Platform ICMS, which stores KV cache for contexts that allow future reuse.\nMoreover, NVIDIA announced the acqui-hire of Groq, a company known for its high-performance AI accelerators. Utilizing SRAM extensively, Groq\u0026rsquo;s LPU architecture is highly optimized for low-latency inference. making it a perfect fit for real-time applications. This acquisition is expected to enhance NVIDIA\u0026rsquo;s capabilities in delivering fast and efficient LLM inference.\nImmediate Comments on the News From the authors\u0026rsquo; point of view, the new Rubin architecture is basically adding a KV cache layer to the existing GPU platform. The ideas are similar to the previous post about LMCache, but NVidia have also added some additional hardware components such as the DPUs (Data Processing Unit).\nOn the Groq side, this acqui-hire may be a defensive move for NVidia to reduce competitors. But regardless, ultra-low latency inference is crucial for real-time agents. Groq\u0026rsquo;s architecture is well-suited for this purpose and potentially can be integrated into future NVIDIA platforms.\nThese movements demonstrate NVidia\u0026rsquo;s investments into inference, which it has been talking about since 2025. This aligns with the problem faced by NVidia\u0026rsquo;s biggest end customer: Frontier Labs including OpenAI, Anthropic, \u0026hellip; One of the main doubts for these frontier labs before they raise more capital is their profitability. While people are racing for the best capability on benchmarks and providing training services like Tinker, so far inference is the only cash cow for them. Gross margin of Anthropic and OpenAI are rumored to be around 40% according to The Information, leaving plenty of room for improvement.\nImportance of KV Cache Hits In our previous blog post, we disussed the significance of KV cache hits in enhancing the efficiency of LLM agents. By storing previously computed KV Cache, agents can avoid redundant computations, leading to faster response times and reduced computational load.\nThe danger of a cache non-hit for any agent LLM trace is that it has to do the prefill for all the previous tokens again. For example, if an agent misses the previous KV cache for a 100K token context, it has to redo the prefill. On xx GPU with balba, prefill takes xx seconds. Since prefill is not batched, this will also block other tasks on the same GPU from being scheduled. Given that each agent can easily go over 20 turns, a few times of KV cache re-prefill can easily inflate the total latency by multiple times.\nHowever, if we are able to save and reuse the KV cache for long contexts, we can significantly reduce the full prefill time. For each round of the agents, we only need to do incremental prefill for the newly added user and agent messages. This can lead to substantial speedups by reduced computation.\nStory of Decoding Speed and the Memory Bottleneck However, we did not go into full depth of decoding in agent inference.\nAgents on many categories often have . We give a brief breakdown of the time spent in prefill and decoding for a SWE-agent task assuming this is the only task on the set of GPUs. Although decoding is often executed in batched manner to increase total throughput of the system, the latency experienced by each individual user will still be very long if there are many decoding tokens.\nBelow is a breakdown of time spent in prefill and decoding for a SWE-agent task assuming this is the only task on the set of GPUs.\nThe key bottleneck for decoding speed is the memory access pattern. During decoding, the model needs to frequently load the model weights as well as KV cache. This results in a minimal delay of total_memory / HBM bandwidth for each generated token. Below is a graph on the roofline model for GPU computations. Decoding falls into the memory bound regime, meaning that the execution time is dominated by memory access rather than how fast you can do the computations like matrix multiplication.\nAdmittedly, speculative decoding as discussed in this blog can help reduce the number of memory accesses by generating multiple tokens in one forward pass. However, the fundamental bottleneck remains: each forward pass still requires loading weights and KV cache from memory.\nGroq\u0026rsquo;s LPU architecture, which utilizes SRAM for storing model weights, offers a potential solution to this bottleneck. SRAM provides significantly faster access times compared to traditional memory types like HBM or DDR by directly putting memory on chip. This allows them to have 80TB/s bandwidth for SRAM as discussed by the blog. This allows them to have much faster vanilla (without speculative decoding) speed compared to traditional GPU architectures.\nWhy NVidia has the Potential to be the First to achieve real-time agents Let\u0026rsquo;s now assume we are the CTO of NVidia. Given the previous calculations, you will suddenly realize that if we just put the two pieces together: ICMS for KV cache hits and Groq\u0026rsquo;s SRAM-based architecture for decoding speed, we can potentially achieve real-time agents. Here is a rough sketch of the proposed architecture:\nOverall, the architecture we are proposing two components:\nPrefill node, which uses the compute optimized GPUs as compute units for incremental prefill and connects to many storage devices to store KV cache for long contexts. Decoding node, which uses specialized hardware that opimizes for memory access to achieve fast decoding. Below, we will do a simulation demonstrating how this architecture can potentially speed up agent inference from over minutes (now) to real-time (future).\nChallenges for Realizing the Proposed Architecture We outline some of the immediate challenges that come to authors\u0026rsquo; minds for realizing the proposed architecture:\nThe integration between NVidia\u0026rsquo;s GPU platform and Groq\u0026rsquo;s LPU architecture. This includes architecture design, data transfer protocols, and other compatibility issues. Author does not work on hardware design and thus cannot comment too much on the specific hardware. But it remains unclear how we can expose software APIs to allow seamless data transfer between the two hardwares.\nThe software stack for coordinating prefill and decoding nodes. Although the idea of ICMS is promising, software stack for efficiently coordinating between prefill and decoding nodes is non-trivial. This includes design of different caching policies, scheduling systems for balancing delay and throughput under agentic scenarios, and other system-level optimizations. There have been initial effort on software stack for LLM inference such as vLLM, SGLang, LMCache\u0026hellip; But customizing them for these specialized hardware workloads will remain challenging. Moreover, utilizing speculative decoding in the new architecture will also require significant software engineering efforts.\nConclusion The convergence of NVidia\u0026rsquo;s massive compute scale with Groq\u0026rsquo;s specialized decoding speed brings many future opportunites. By combining the newly released inference context management storage system for reusing KV cache and groq\u0026rsquo;s SRAM for rapid decoding, NVidia potentially can overcome the latency bottlenecks that currently plague complex agentic workflows. While our proposed architecture is theoretical, the strategic moves at CES 2026 suggest this is the direction the industry is heading. Real-time, context-aware agents are no longer just a software ambition; the hardware foundation is finally being laid to make them a reality.\n","permalink":"http://localhost:1313/blog/posts/fast_agent/","summary":"\u003cp\u003eIn this blog post, we will discuss NVidia\u0026rsquo;s recent announcements at CES and their acquisition of Groq, focusing on their strategy to enhance LLM agent inference.\nWe will explore three main aspects: the importance of KV cache hits, the role of SRAM in improving decoding speed, and a proposed hardware-software architecture that potentially speeds up agent inference from task-based to real-time.\u003c/p\u003e\n\u003cp\u003ePrerequisite: LLM inference basics.\nSuggested reading:\n\u003ca href=\"https://arpitbhayani.me/blogs/how-llm-inference-works/\"\u003eLLM Inference\u003c/a\u003e;   \u003ca href=\"https://blog.lmcache.ai/en/2024/09/17/lmcache-turboboosting-vllm-with-7x-faster-access-to-100x-more-kv-caches/\"\u003eKV Cache Offloading with LMCache\u003c/a\u003e; \u003ca href=\"https://hanchenli.github.io/blog/posts/kv_agent/\"\u003eLLM Agent with KV Cache\u003c/a\u003e\u003c/p\u003e","title":"CES and Groq Acqui-hire Reflection: NVidia's Potential to Build Real Time Agents"},{"content":"Agents are EXPENSIVE. Claude Code takes 1 USD to handle a single issue in a mid-sized public repository when using API. In the same time, it charges 20 USD per month for a subscription license. Why do we still get to use them? Maybe the price war, maybe the crazy debts some companies upstream are carrying, maybe the implicit labeling you are carrying out for the providers.\nBut regardless of the reason behind the pricing, we want to use more these powerful agents. Current models and applications are already capable of handling many complex tasks with minimal human intervention. However, the efficiency of these agents has just rised to our attention. In this blog post, we will discuss why agents are efficiency nightmares, how we can make them (somewhat) better with KV cache management tool like LMCache, and where we could be moving forward for improving agents.\nPrerequisite: LLM inference basics. Suggested reading: LLM Inference; KV Cache Offloading with LMCache\nPowerful Agents; But Draining Bank Account. Everyone said 2025 was the year of agents. With continuous improment in base model and emerging techniques like reinforcement learning, coding agents including Cursor, Claude Code have become much more powerful and automated compared with the start of the year. Quantitatively, the score on SWE-Bench has risen from 20% as in Aug 2024 to over 70% for frontier models with a simple agent scaffold.\nHowever, the cost of running these agents is still prohibitively high. We conducted a simple benchmark experiment running Claude Code with APIs to get a quantitative measure of the actual token cost. Our experiment with 30 random verified tasks from SWE-bench revealed some eye-opening numbers. The agent achieved a 63.3% resolution rate, successfully fixing 19 out of 30 complex repository issues autonomously. However, this performance comes with a cost: while the average expense was $0.74 per issue, a single complex Django bug could spike the cost to over $3.20. Most interestingly, over 90% of the token volume was spent on \u0026lsquo;cache reads,\u0026rsquo; indicating that the agent spent the vast majority of its budget repeatedly re-scanning the codebase to verify its steps.\nWe found that handling a single issue in a mid-sized public repository costs around 1 USD when using Claude Code via API. This is in stark contrast to the subscription license cost of 20 USD per month. There could be various reasons behind this pricing discrepancy, such as market dynamics, provider strategies, or the implicit value of the labeling work being done by the users. However, despite the high costs, the capabilities of these agents make them indispensable for many users. As proved by the usage-based pricing model adopted by Manus, many users are willing to pay for the advanced functionalities provided by these agents, even at a premium.\nHowever, as the adoption of these agents grows, the efficiency of their operation becomes increasingly important. This directly improves the Return-On-Investment (ROI) for users and makes these tools more accessible to a broader audience.\nThe Fundamental Inefficiency of Agents In order to understand why agents are so expensive to run, we need to look at how they operate fundamentally. We analyze trace from Claude Code using Claude-4.5-sonnet and mini-swe-agent using GPT-5 to give a brief overview. We used each agent to solve a coding task from SWE-bench_Verified dataset and recorded the message history sent. The cost report is based on token usage from the API responses and actual bills.\nOverview Statistics We report some high-level statistics from the traces below:\nMethod Average Total Tokens Average Cost ($) Average Calls to LLM Prefix Cache Hit Rate Claude Code 1,765,565.63 0.74 74.17 96.3% Mini-SWE-Agent 230,709.64 0.192 17.916 94.559% Microscopic View We randomly select one task (#80) from the SWE-bench_Verified dataset. The problem setup is to fix an issue in the django/django repo from commit 2e0f04507b17362239ba49830d26fec504d46978.\nProblem statement:\n\u0026ldquo;JSONField are not properly displayed in admin when they are readonly.\nDescription: JSONField values are displayed as dict when readonly in the admin. For example, {\u0026quot;foo\u0026quot;: \u0026quot;bar\u0026quot;} would be displayed as {'foo': 'bar'}, which is not valid JSON.\nI believe the fix would be to add a special case in django.contrib.admin.utils.display_for_field to call the prepare_value of the JSONField (not calling json.dumps directly to take care of the InvalidJSONInput case).\u0026rdquo;\nAnd this is exactly the prompt that Claude Code received.\nSurprisingly, before any fancy reasoning, Claude Code ran a couple of \u0026ldquo;warm-up\u0026rdquo; steps (trace ID #2, #3, #4) before the actual task. Warm-up steps do nothing but input the prompt for:\nTool list (#2) Explore subagent (#3) Plan subagent (#4) Warm-up steps are used for caching purposes—later when those tools and subagents are called, the cache will be hit, resulting in faster response time. The summarization agent (#1) and new topic agent (#5) are used for summarizing the context and generating a new title for display—just as the ChatGPT sidebar works.\nThe main agent (#6) comes with a huge system prompt, including git history, status, tool list, etc. The 18 tools in the tool list not only have the ability to use normal tool calls like Bash, Grep, Read, WebFetch, AskUserQuestion, etc., but also the ability to invoke and delegate certain tasks to subagents like:\nExplore subagent (#7) Plan subagent (#46) These subagents will invoke tool calls from their own tool lists.\nImmediately after the main agent (#6), it invokes the Explore (also called file search agent) subagent (#7), which will invoke tool calls from its tool list to explore the codebase. It starts with a different system prompt where its main goal is to explore the codebase:\nYou are Claude Code, Anthropic\u0026rsquo;s official CLI for Claude. You are a file search specialist for Claude Code, Anthropic\u0026rsquo;s official CLI for Claude. You excel at thoroughly navigating and exploring codebases.\nInterestingly, the Explore subagent (#7) is not the only subagent that Claude Code can invoke. Instead, it invokes 3 Explore subagents in parallel to explore the codebase, each with a different goal:\nExplore JSONField implementation (lifespan: #7-#26) Explore admin display_for_field (lifespan: #8-#37) Explore readonly field rendering (lifespan: #9-#45) The context of the main agent (#6) is not carried to the subagents, which is beneficial for the subagents to have a fresh start. Each Explore subagent can invoke 1-3 tools in parallel, where the tools are from the tool list of the Explore subagent—a subset (10/18) of the main agent\u0026rsquo;s tool list.\nThe ReAct mechanism is used here: the Explore subagent will invoke a tool call, then based on the tool output, it will observe and invoke another tool call to explore the codebase further until it deems it has explored enough.\nFinally, after the slowest Explore subagent finishes its exploration at step #45, at step #46, the main agent appends the findings (summarizations) from all 3 Explore subagents to the context, and then invokes the Plan subagent (#47) to plan the fix.\nSimilar to the Explore Agent, the Plan Agent (#47) also has a different system prompt, where its main goal is to plan the fix:\nYou are Claude Code, Anthropic\u0026rsquo;s official CLI for Claude. You are a software architect and planning specialist for Claude Code. Your role is to explore the codebase and design implementation plans.\nThe Plan Agent did not carry all the context from the main agent nor the Explore subagents, which is beneficial for the Plan Agent to have a fresh start. Instead, it only contains the summarization of the Explore subagents\u0026rsquo; findings. The toolbox is a subset (10/18) of the main agent\u0026rsquo;s tool list. The goal for the Plan Agent is to design an implementation plan that:\nPlease design an implementation plan that:\nIdentifies the exact changes needed to display_for_field Considers whether we need to instantiate a form field from the model field or if there\u0026rsquo;s a better approach Identifies any edge cases or potential issues Recommends the best approach given Django\u0026rsquo;s architecture Similarly, the Plan Agent also follows the ReAct pattern and loops through tool calling from #47 to #72, where the context accumulates from 11,552 tokens to 38,819 tokens. After having a good plan (see details in #72), the Plan Agent will return to the main agent (#73) with the plan.\nThe main agent will then invoke a series of tool calls to:\nReview the plan (#73) Ask user for clarification (#74) Write the plan into a markdown file (#75) Finally, the main agent will exit the plan mode (#76) and enter the execute mode (#77) to execute the plan after interactively asking the user for plan approval (#76-#77).\nThe execution phase (#77-#91) still follows the ReAct pattern. The main agent will use the plan markdown file as a todo list:\nAdd json import to utils.py Add JSONField handling to display_for_field() Add tests to test_admin_utils.py Run the tests to verify After executing some tool calls to read or edit files, it will cross out the todo items in the plan markdown file. Once all the todo items are crossed out, the main agent will end with a conclusion message (#92).\nDuring this phase, there are some other subagents being invoked—e.g., the Extract Bash Command subagent (#93), where there\u0026rsquo;s only a one-shot prompt template for the subagent to extract the bash command in order to not run dangerous commands like rm without user confirmation by accident.\nAnd this is the whole diagram of the claude code trace:\nKey Observations We highlight three key observations from the trace analysis above that contribute to the inefficiency of agents:\nHigh number of calls: Agents make a large number of calls to the LLM server due to the need to repetitive get actions and report status. Each call incurs incremental prefill and may cause KV cache miss that requires recomputation. Appending Context: Agents often append new context to the existing conversation history, leading to longer input sequences and increased computational load for each call. Fundamentally, due to the need to fully incorporate the context of the task into the LLM inputs and the heavy intermediate states (KV cache) related to these contexts, agents are highly memory-bound workloads. Sadly, memory bandwidth and capacity have not improved as fast as compute (FLOPS) in recent years.\nWhy is Current KV Cache Offloading Not Enough? From the table above, we can see that both agents have a high prefix cache hit rate (around 90%). Readers with background may wonder: 90+% Prefix Cache hit rate? Why not just run KV Cache Offloading and save 10x?\nHowever, there are three further considerations here:\nContention for Space: Due to the appending context behavior, the KV cache space is highly contended among different agent programs due to long context. Queueling Delay in Scheduling: While the current agent program is waiting on the tool, if the sched uler allocates the GPU memory to other requests to maximize throughput, the KV cache for the current program will be removed from GPU memory. This incurs a queueing delay for each request when the program resumes after tool execution. Remaining Compute: As mentioned above, each call still requires prefill compute even with KV cache hit. With a high number of calls, the remaining compute becomes non-negligible. These issue limits agents from fully benefitting from KV cache offloading. For example, in the graph above, we enabled vLLM with LMCache to serve agent traces under a certain jps. However, due to the high contention and queueling delay, each agent still incurs significant waiting time even with KV cache offloading enabled.\nSome research works have demonstrated by mitigating the first two issues, KV cache offloading can bring significant speedup and cost reduction for serving LLMs.\nFor example, Continuum proposes to reuse the Time-to-live (TTL) concept to preserve the KV cache for programs that are likely to be resumed soon to mitigate the first two problems. This changes the eviction policy from LRU to TTL-based, which is more suitable for agent workloads.\nAs demonstrated by the experiments, Continuum can bring up to 3.66x improvement for serving LLM agent traces with long context on SWE-Bench and Berkeley Function Calling Leaderboard workloads by improving upon naive KV cache offloading. The above graph shows the evaluations results. In the graph, each method represents a scheduling policy using vLLM and using KV cache offloading with LMCache. For A100 GPUs, we pair 100GB DRAM per GPU and for B200 GPUs, we pair 200GB DRAM per GPU. We set TP=4 for 70B model. The requests arrive according to a Poisson process with a certain JPS. The graphs show the average latency per request under different JPS.\nThe paper arxiv links is here: Continuum Paper, and a preview version of code is available at: Continuum Code.\nLooking Forward: Beyond KV Cache Management However, as shown in the graphs, each agent execution still takes significant time even with KV cache offloading. Even without any contention, each call still requires multiple long prefill and decode operations throughput the trace. These computations add up in the end.\nTo further improve the efficiency of agents, we need to look beyond just KV cache management. Some potential directions include:\nModel Optimization: Researchers have been proposing methods that reduces memory footprint for intermediate states recorded in the agent trace. Examples include KV cache compressions, and more efficient attention mechanisms like linear attention, Mamba mechanisms. Adaptive Context Management: As a recent trend, agentic context engineering (ACE) has been growing popularity. Many work proposes to filter the input text before feeding them into the LLM. Example works include ReSum, AgentFold, and Manus\u0026rsquo;s Context Engineering Report\u0026hellip; These methods prevents unncessary context from flooding the LLM context window, reducing computation from the source. Going forward, agents are and will probably remain memory-bound for the foreseeable future. Addressing this fundamental bottleneck requires a careful co-design of algorithms and systems. This enables not only powerful but efficient future agents. After all, ROI is the key metric for GenAI to succeed in the real industry in the long run.\nConclusion In this blog post, we discussed why these powerful agents are actually efficiency nightmares. The agents frequent sends LLM requests, have long context histories leading to large intermediate states, and incurs heavy use of GPU resources. Prefix caching provides a promising solution to prevent repetitive prefill but naive policies may not be sufficient. We demonstrated how KV cache sharing techniques like Continuum can help mitigate some of the inefficiencies. However, we also highlighted that further improvements are needed beyond KV cache management due to the memory-bound nature of long context inference, including model optimizations and adaptive context management strategies.\nAcknowledgements We would like to thank authors of Continuum and folks from LMCache and Tensormesh for their valuable discussions and feedback on this blog post.\nDisclaimer: This blog post was created with help from Gemini, VSCode copilot. Views are solely from the authors and do not reflect employer values.\n","permalink":"http://localhost:1313/blog/posts/kv_agent/","summary":"\u003cp\u003eAgents are EXPENSIVE. Claude Code takes 1 USD to handle a single issue in a mid-sized public repository when using API.\nIn the same time, it charges 20 USD per month for a subscription license.\nWhy do we still get to use them?\nMaybe the price war, maybe the crazy debts some companies upstream are carrying, maybe the implicit labeling you are carrying out for the providers.\u003c/p\u003e\n\u003cp\u003eBut regardless of the reason behind the pricing, we want to use more these powerful agents.\nCurrent models and applications are already capable of handling many complex tasks with minimal human intervention.\nHowever, the efficiency of these agents has just rised to our attention.\nIn this blog post, we will discuss why agents are efficiency nightmares, how we can make them (somewhat) better with KV cache management tool like LMCache,\nand where we could be moving forward for improving agents.\u003c/p\u003e","title":"Why Agents are Efficiency Nightmares and How to Fix them?"},{"content":"In this blog post, we will discuss NVidia\u0026rsquo;s recent announcements at CES and their acquisition of Groq, focusing on their strategy to enhance LLM agent inference. We will explore three main aspects: the importance of KV cache hits, the role of SRAM in improving decoding speed, and a proposed hardware-software architecture that potentially speeds up agent inference from task-based to real-time.\nPrerequisite: LLM inference basics. Suggested reading: LLM Inference; KV Cache Offloading with LMCache; LLM Agent with KV Cache\nCES Announcements and Groq Acquisition In CES 2026, NVIDIA introduced the Rubin architecture. As described by this announcement This new GPU architecture is designed to significantly boost the performance of large language model (LLM) inference by 10x. It incorporates NVIDIA Vera CPU, Rubin GPU, NVLink 6 Switch, ConnectX-9 SuperNIC, BlueField-4 DPU and Spectrum-6 Ethernet Switch. It natively supports the Inference Context Memory Storage Platform ICMS, which stores KV cache for contexts that allow future reuse.\nMoreover, NVIDIA announced the acqui-hire of Groq, a company known for its high-performance AI accelerators. Utilizing SRAM extensively, Groq\u0026rsquo;s LPU architecture is highly optimized for low-latency inference. making it a perfect fit for real-time applications. This acquisition is expected to enhance NVIDIA\u0026rsquo;s capabilities in delivering fast and efficient LLM inference.\nImmediate Comments on the News From the authors\u0026rsquo; point of view, the new Rubin architecture is basically adding a KV cache layer to the existing GPU platform. The ideas are similar to the previous post about LMCache, but NVidia have also added some additional hardware components such as the DPUs (Data Processing Unit).\nOn the Groq side, this acqui-hire may be a defensive move for NVidia to reduce competitors. But regardless, ultra-low latency inference is crucial for real-time agents. Groq\u0026rsquo;s architecture is well-suited for this purpose and potentially can be integrated into future NVIDIA platforms.\nThese movements demonstrate NVidia\u0026rsquo;s investments into inference, which it has been talking about since 2025. This aligns with the problem faced by NVidia\u0026rsquo;s biggest end customer: Frontier Labs including OpenAI, Anthropic, \u0026hellip; One of the main doubts for these frontier labs before they raise more capital is their profitability. While people are racing for the best capability on benchmarks and providing training services like Tinker, so far inference is the only cash cow for them. Gross margin of Anthropic and OpenAI are rumored to be around 40% according to The Information, leaving plenty of room for improvement.\nImportance of KV Cache Hits In our previous blog post, we disussed the significance of KV cache hits in enhancing the efficiency of LLM agents. By storing previously computed KV Cache, agents can avoid redundant computations, leading to faster response times and reduced computational load.\nThe danger of a cache non-hit for any agent LLM trace is that it has to do the prefill for all the previous tokens again. For example, if an agent misses the previous KV cache for a 100K token context, it has to redo the prefill. On xx GPU with balba, prefill takes xx seconds. Since prefill is not batched, this will also block other tasks on the same GPU from being scheduled. Given that each agent can easily go over 20 turns, a few times of KV cache re-prefill can easily inflate the total latency by multiple times.\nHowever, if we are able to save and reuse the KV cache for long contexts, we can significantly reduce the full prefill time. For each round of the agents, we only need to do incremental prefill for the newly added user and agent messages. This can lead to substantial speedups by reduced computation.\nStory of Decoding Speed and the Memory Bottleneck However, we did not go into full depth of decoding in agent inference.\nAgents on many categories often have . We give a brief breakdown of the time spent in prefill and decoding for a SWE-agent task assuming this is the only task on the set of GPUs. Although decoding is often executed in batched manner to increase total throughput of the system, the latency experienced by each individual user will still be very long if there are many decoding tokens.\nBelow is a breakdown of time spent in prefill and decoding for a SWE-agent task assuming this is the only task on the set of GPUs.\nThe key bottleneck for decoding speed is the memory access pattern. During decoding, the model needs to frequently load the model weights as well as KV cache. This results in a minimal delay of total_memory / HBM bandwidth for each generated token. Below is a graph on the roofline model for GPU computations. Decoding falls into the memory bound regime, meaning that the execution time is dominated by memory access rather than how fast you can do the computations like matrix multiplication.\nAdmittedly, speculative decoding as discussed in this blog can help reduce the number of memory accesses by generating multiple tokens in one forward pass. However, the fundamental bottleneck remains: each forward pass still requires loading weights and KV cache from memory.\nGroq\u0026rsquo;s LPU architecture, which utilizes SRAM for storing model weights, offers a potential solution to this bottleneck. SRAM provides significantly faster access times compared to traditional memory types like HBM or DDR by directly putting memory on chip. This allows them to have 80TB/s bandwidth for SRAM as discussed by the blog. This allows them to have much faster vanilla (without speculative decoding) speed compared to traditional GPU architectures.\nWhy NVidia has the Potential to be the First to achieve real-time agents Let\u0026rsquo;s now assume we are the CTO of NVidia. Given the previous calculations, you will suddenly realize that if we just put the two pieces together: ICMS for KV cache hits and Groq\u0026rsquo;s SRAM-based architecture for decoding speed, we can potentially achieve real-time agents. Here is a rough sketch of the proposed architecture:\nOverall, the architecture we are proposing two components:\nPrefill node, which uses the compute optimized GPUs as compute units for incremental prefill and connects to many storage devices to store KV cache for long contexts. Decoding node, which uses specialized hardware that opimizes for memory access to achieve fast decoding. Below, we will do a simulation demonstrating how this architecture can potentially speed up agent inference from over minutes (now) to real-time (future).\nChallenges for Realizing the Proposed Architecture We outline some of the immediate challenges that come to authors\u0026rsquo; minds for realizing the proposed architecture:\nThe integration between NVidia\u0026rsquo;s GPU platform and Groq\u0026rsquo;s LPU architecture. This includes architecture design, data transfer protocols, and other compatibility issues. Author does not work on hardware design and thus cannot comment too much on the specific hardware. But it remains unclear how we can expose software APIs to allow seamless data transfer between the two hardwares.\nThe software stack for coordinating prefill and decoding nodes. Although the idea of ICMS is promising, software stack for efficiently coordinating between prefill and decoding nodes is non-trivial. This includes design of different caching policies, scheduling systems for balancing delay and throughput under agentic scenarios, and other system-level optimizations. There have been initial effort on software stack for LLM inference such as vLLM, SGLang, LMCache\u0026hellip; But customizing them for these specialized hardware workloads will remain challenging. Moreover, utilizing speculative decoding in the new architecture will also require significant software engineering efforts.\nConclusion The convergence of NVidia\u0026rsquo;s massive compute scale with Groq\u0026rsquo;s specialized decoding speed brings many future opportunites. By combining the newly released inference context management storage system for reusing KV cache and groq\u0026rsquo;s SRAM for rapid decoding, NVidia potentially can overcome the latency bottlenecks that currently plague complex agentic workflows. While our proposed architecture is theoretical, the strategic moves at CES 2026 suggest this is the direction the industry is heading. Real-time, context-aware agents are no longer just a software ambition; the hardware foundation is finally being laid to make them a reality.\n","permalink":"http://localhost:1313/blog/posts/fast_agent/","summary":"\u003cp\u003eIn this blog post, we will discuss NVidia\u0026rsquo;s recent announcements at CES and their acquisition of Groq, focusing on their strategy to enhance LLM agent inference.\nWe will explore three main aspects: the importance of KV cache hits, the role of SRAM in improving decoding speed, and a proposed hardware-software architecture that potentially speeds up agent inference from task-based to real-time.\u003c/p\u003e\n\u003cp\u003ePrerequisite: LLM inference basics.\nSuggested reading:\n\u003ca href=\"https://arpitbhayani.me/blogs/how-llm-inference-works/\"\u003eLLM Inference\u003c/a\u003e;   \u003ca href=\"https://blog.lmcache.ai/en/2024/09/17/lmcache-turboboosting-vllm-with-7x-faster-access-to-100x-more-kv-caches/\"\u003eKV Cache Offloading with LMCache\u003c/a\u003e; \u003ca href=\"https://hanchenli.github.io/blog/posts/kv_agent/\"\u003eLLM Agent with KV Cache\u003c/a\u003e\u003c/p\u003e","title":"CES and Groq Acqui-hire Reflection: NVidia's Potential to Build Real Time Agents"},{"content":"Agents are EXPENSIVE. Claude Code takes 1 USD to handle a single issue in a mid-sized public repository when using API. In the same time, it charges 20 USD per month for a subscription license. Why do we still get to use them? Maybe the price war, maybe the crazy debts some companies upstream are carrying, maybe the implicit labeling you are carrying out for the providers.\nBut regardless of the reason behind the pricing, we want to use more these powerful agents. Current models and applications are already capable of handling many complex tasks with minimal human intervention. However, the efficiency of these agents has just rised to our attention. In this blog post, we will discuss why agents are efficiency nightmares, how we can make them (somewhat) better with KV cache management tool like LMCache, and where we could be moving forward for improving agents.\nPrerequisite: LLM inference basics. Suggested reading: LLM Inference; KV Cache Offloading with LMCache\nPowerful Agents; But Draining Bank Account. Everyone said 2025 was the year of agents. With continuous improment in base model and emerging techniques like reinforcement learning, coding agents including Cursor, Claude Code have become much more powerful and automated compared with the start of the year. Quantitatively, the score on SWE-Bench has risen from 20% as in Aug 2024 to over 70% for frontier models with a simple agent scaffold.\nHowever, the cost of running these agents is still prohibitively high. We conducted a simple benchmark experiment running Claude Code with APIs to get a quantitative measure of the actual token cost. Our experiment with 30 random verified tasks from SWE-bench revealed some eye-opening numbers. The agent achieved a 63.3% resolution rate, successfully fixing 19 out of 30 complex repository issues autonomously. However, this performance comes with a cost: while the average expense was $0.74 per issue, a single complex Django bug could spike the cost to over $3.20. Most interestingly, over 90% of the token volume was spent on \u0026lsquo;cache reads,\u0026rsquo; indicating that the agent spent the vast majority of its budget repeatedly re-scanning the codebase to verify its steps.\nWe found that handling a single issue in a mid-sized public repository costs around 1 USD when using Claude Code via API. This is in stark contrast to the subscription license cost of 20 USD per month. There could be various reasons behind this pricing discrepancy, such as market dynamics, provider strategies, or the implicit value of the labeling work being done by the users. However, despite the high costs, the capabilities of these agents make them indispensable for many users. As proved by the usage-based pricing model adopted by Manus, many users are willing to pay for the advanced functionalities provided by these agents, even at a premium.\nHowever, as the adoption of these agents grows, the efficiency of their operation becomes increasingly important. This directly improves the Return-On-Investment (ROI) for users and makes these tools more accessible to a broader audience.\nThe Fundamental Inefficiency of Agents In order to understand why agents are so expensive to run, we need to look at how they operate fundamentally. We analyze trace from Claude Code using Claude-4.5-sonnet and mini-swe-agent using GPT-5 to give a brief overview. We used each agent to solve a coding task from SWE-bench_Verified dataset and recorded the message history sent. The cost report is based on token usage from the API responses and actual bills.\nOverview Statistics We report some high-level statistics from the traces below:\nMethod Average Total Tokens Average Cost ($) Average Calls to LLM Prefix Cache Hit Rate Claude Code 1,765,565.63 0.74 74.17 96.3% Mini-SWE-Agent 230,709.64 0.192 17.916 94.559% Microscopic View We randomly select one task (#80) from the SWE-bench_Verified dataset. The problem setup is to fix an issue in the django/django repo from commit 2e0f04507b17362239ba49830d26fec504d46978.\nProblem statement:\n\u0026ldquo;JSONField are not properly displayed in admin when they are readonly.\nDescription: JSONField values are displayed as dict when readonly in the admin. For example, {\u0026quot;foo\u0026quot;: \u0026quot;bar\u0026quot;} would be displayed as {'foo': 'bar'}, which is not valid JSON.\nI believe the fix would be to add a special case in django.contrib.admin.utils.display_for_field to call the prepare_value of the JSONField (not calling json.dumps directly to take care of the InvalidJSONInput case).\u0026rdquo;\nAnd this is exactly the prompt that Claude Code received.\nSurprisingly, before any fancy reasoning, Claude Code ran a couple of \u0026ldquo;warm-up\u0026rdquo; steps (trace ID #2, #3, #4) before the actual task. Warm-up steps do nothing but input the prompt for:\nTool list (#2) Explore subagent (#3) Plan subagent (#4) Warm-up steps are used for caching purposes—later when those tools and subagents are called, the cache will be hit, resulting in faster response time. The summarization agent (#1) and new topic agent (#5) are used for summarizing the context and generating a new title for display—just as the ChatGPT sidebar works.\nThe main agent (#6) comes with a huge system prompt, including git history, status, tool list, etc. The 18 tools in the tool list not only have the ability to use normal tool calls like Bash, Grep, Read, WebFetch, AskUserQuestion, etc., but also the ability to invoke and delegate certain tasks to subagents like:\nExplore subagent (#7) Plan subagent (#46) These subagents will invoke tool calls from their own tool lists.\nImmediately after the main agent (#6), it invokes the Explore (also called file search agent) subagent (#7), which will invoke tool calls from its tool list to explore the codebase. It starts with a different system prompt where its main goal is to explore the codebase:\nYou are Claude Code, Anthropic\u0026rsquo;s official CLI for Claude. You are a file search specialist for Claude Code, Anthropic\u0026rsquo;s official CLI for Claude. You excel at thoroughly navigating and exploring codebases.\nInterestingly, the Explore subagent (#7) is not the only subagent that Claude Code can invoke. Instead, it invokes 3 Explore subagents in parallel to explore the codebase, each with a different goal:\nExplore JSONField implementation (lifespan: #7-#26) Explore admin display_for_field (lifespan: #8-#37) Explore readonly field rendering (lifespan: #9-#45) The context of the main agent (#6) is not carried to the subagents, which is beneficial for the subagents to have a fresh start. Each Explore subagent can invoke 1-3 tools in parallel, where the tools are from the tool list of the Explore subagent—a subset (10/18) of the main agent\u0026rsquo;s tool list.\nThe ReAct mechanism is used here: the Explore subagent will invoke a tool call, then based on the tool output, it will observe and invoke another tool call to explore the codebase further until it deems it has explored enough.\nFinally, after the slowest Explore subagent finishes its exploration at step #45, at step #46, the main agent appends the findings (summarizations) from all 3 Explore subagents to the context, and then invokes the Plan subagent (#47) to plan the fix.\nSimilar to the Explore Agent, the Plan Agent (#47) also has a different system prompt, where its main goal is to plan the fix:\nYou are Claude Code, Anthropic\u0026rsquo;s official CLI for Claude. You are a software architect and planning specialist for Claude Code. Your role is to explore the codebase and design implementation plans.\nThe Plan Agent did not carry all the context from the main agent nor the Explore subagents, which is beneficial for the Plan Agent to have a fresh start. Instead, it only contains the summarization of the Explore subagents\u0026rsquo; findings. The toolbox is a subset (10/18) of the main agent\u0026rsquo;s tool list. The goal for the Plan Agent is to design an implementation plan that:\nPlease design an implementation plan that:\nIdentifies the exact changes needed to display_for_field Considers whether we need to instantiate a form field from the model field or if there\u0026rsquo;s a better approach Identifies any edge cases or potential issues Recommends the best approach given Django\u0026rsquo;s architecture Similarly, the Plan Agent also follows the ReAct pattern and loops through tool calling from #47 to #72, where the context accumulates from 11,552 tokens to 38,819 tokens. After having a good plan (see details in #72), the Plan Agent will return to the main agent (#73) with the plan.\nThe main agent will then invoke a series of tool calls to:\nReview the plan (#73) Ask user for clarification (#74) Write the plan into a markdown file (#75) Finally, the main agent will exit the plan mode (#76) and enter the execute mode (#77) to execute the plan after interactively asking the user for plan approval (#76-#77).\nThe execution phase (#77-#91) still follows the ReAct pattern. The main agent will use the plan markdown file as a todo list:\nAdd json import to utils.py Add JSONField handling to display_for_field() Add tests to test_admin_utils.py Run the tests to verify After executing some tool calls to read or edit files, it will cross out the todo items in the plan markdown file. Once all the todo items are crossed out, the main agent will end with a conclusion message (#92).\nDuring this phase, there are some other subagents being invoked—e.g., the Extract Bash Command subagent (#93), where there\u0026rsquo;s only a one-shot prompt template for the subagent to extract the bash command in order to not run dangerous commands like rm without user confirmation by accident.\nAnd this is the whole diagram of the claude code trace:\nKey Observations We highlight three key observations from the trace analysis above that contribute to the inefficiency of agents:\nHigh number of calls: Agents make a large number of calls to the LLM server due to the need to repetitive get actions and report status. Each call incurs incremental prefill and may cause KV cache miss that requires recomputation. Appending Context: Agents often append new context to the existing conversation history, leading to longer input sequences and increased computational load for each call. Fundamentally, due to the need to fully incorporate the context of the task into the LLM inputs and the heavy intermediate states (KV cache) related to these contexts, agents are highly memory-bound workloads. Sadly, memory bandwidth and capacity have not improved as fast as compute (FLOPS) in recent years.\nWhy is Current KV Cache Offloading Not Enough? From the table above, we can see that both agents have a high prefix cache hit rate (around 90%). Readers with background may wonder: 90+% Prefix Cache hit rate? Why not just run KV Cache Offloading and save 10x?\nHowever, there are three further considerations here:\nContention for Space: Due to the appending context behavior, the KV cache space is highly contended among different agent programs due to long context. Queueling Delay in Scheduling: While the current agent program is waiting on the tool, if the sched uler allocates the GPU memory to other requests to maximize throughput, the KV cache for the current program will be removed from GPU memory. This incurs a queueing delay for each request when the program resumes after tool execution. Remaining Compute: As mentioned above, each call still requires prefill compute even with KV cache hit. With a high number of calls, the remaining compute becomes non-negligible. These issue limits agents from fully benefitting from KV cache offloading. For example, in the graph above, we enabled vLLM with LMCache to serve agent traces under a certain jps. However, due to the high contention and queueling delay, each agent still incurs significant waiting time even with KV cache offloading enabled.\nSome research works have demonstrated by mitigating the first two issues, KV cache offloading can bring significant speedup and cost reduction for serving LLMs.\nFor example, Continuum proposes to reuse the Time-to-live (TTL) concept to preserve the KV cache for programs that are likely to be resumed soon to mitigate the first two problems. This changes the eviction policy from LRU to TTL-based, which is more suitable for agent workloads.\nAs demonstrated by the experiments, Continuum can bring up to 3.66x improvement for serving LLM agent traces with long context on SWE-Bench and Berkeley Function Calling Leaderboard workloads by improving upon naive KV cache offloading. The above graph shows the evaluations results. In the graph, each method represents a scheduling policy using vLLM and using KV cache offloading with LMCache. For A100 GPUs, we pair 100GB DRAM per GPU and for B200 GPUs, we pair 200GB DRAM per GPU. We set TP=4 for 70B model. The requests arrive according to a Poisson process with a certain JPS. The graphs show the average latency per request under different JPS.\nThe paper arxiv links is here: Continuum Paper, and a preview version of code is available at: Continuum Code.\nLooking Forward: Beyond KV Cache Management However, as shown in the graphs, each agent execution still takes significant time even with KV cache offloading. Even without any contention, each call still requires multiple long prefill and decode operations throughput the trace. These computations add up in the end.\nTo further improve the efficiency of agents, we need to look beyond just KV cache management. Some potential directions include:\nModel Optimization: Researchers have been proposing methods that reduces memory footprint for intermediate states recorded in the agent trace. Examples include KV cache compressions, and more efficient attention mechanisms like linear attention, Mamba mechanisms. Adaptive Context Management: As a recent trend, agentic context engineering (ACE) has been growing popularity. Many work proposes to filter the input text before feeding them into the LLM. Example works include ReSum, AgentFold, and Manus\u0026rsquo;s Context Engineering Report\u0026hellip; These methods prevents unncessary context from flooding the LLM context window, reducing computation from the source. Going forward, agents are and will probably remain memory-bound for the foreseeable future. Addressing this fundamental bottleneck requires a careful co-design of algorithms and systems. This enables not only powerful but efficient future agents. After all, ROI is the key metric for GenAI to succeed in the real industry in the long run.\nConclusion In this blog post, we discussed why these powerful agents are actually efficiency nightmares. The agents frequent sends LLM requests, have long context histories leading to large intermediate states, and incurs heavy use of GPU resources. Prefix caching provides a promising solution to prevent repetitive prefill but naive policies may not be sufficient. We demonstrated how KV cache sharing techniques like Continuum can help mitigate some of the inefficiencies. However, we also highlighted that further improvements are needed beyond KV cache management due to the memory-bound nature of long context inference, including model optimizations and adaptive context management strategies.\nAcknowledgements We would like to thank authors of Continuum and folks from LMCache and Tensormesh for their valuable discussions and feedback on this blog post.\nDisclaimer: This blog post was created with help from Gemini, VSCode copilot. Views are solely from the authors and do not reflect employer values.\n","permalink":"http://localhost:1313/blog/posts/kv_agent/","summary":"\u003cp\u003eAgents are EXPENSIVE. Claude Code takes 1 USD to handle a single issue in a mid-sized public repository when using API.\nIn the same time, it charges 20 USD per month for a subscription license.\nWhy do we still get to use them?\nMaybe the price war, maybe the crazy debts some companies upstream are carrying, maybe the implicit labeling you are carrying out for the providers.\u003c/p\u003e\n\u003cp\u003eBut regardless of the reason behind the pricing, we want to use more these powerful agents.\nCurrent models and applications are already capable of handling many complex tasks with minimal human intervention.\nHowever, the efficiency of these agents has just rised to our attention.\nIn this blog post, we will discuss why agents are efficiency nightmares, how we can make them (somewhat) better with KV cache management tool like LMCache,\nand where we could be moving forward for improving agents.\u003c/p\u003e","title":"Why Agents are Efficiency Nightmares and How to Fix them?"},{"content":"In this blog post, we will discuss NVidia\u0026rsquo;s recent announcements at CES and their acquisition of Groq, focusing on their strategy to enhance LLM agent inference. We will explore three main aspects: the importance of KV cache hits, the role of SRAM in improving decoding speed, and a proposed hardware-software architecture that potentially speeds up agent inference from task-based to real-time.\nPrerequisite: LLM inference basics. Suggested reading: LLM Inference; KV Cache Offloading with LMCache; LLM Agent with KV Cache\nCES Announcements and Groq Acquisition In CES 2026, NVIDIA introduced the Rubin architecture. As described by this announcement This new GPU architecture is designed to significantly boost the performance of large language model (LLM) inference by 10x. It incorporates NVIDIA Vera CPU, Rubin GPU, NVLink 6 Switch, ConnectX-9 SuperNIC, BlueField-4 DPU and Spectrum-6 Ethernet Switch. It natively supports the Inference Context Memory Storage Platform ICMS, which stores KV cache for contexts that allow future reuse.\nMoreover, NVIDIA announced the acqui-hire of Groq, a company known for its high-performance AI accelerators. Utilizing SRAM extensively, Groq\u0026rsquo;s LPU architecture is highly optimized for low-latency inference. making it a perfect fit for real-time applications. This acquisition is expected to enhance NVIDIA\u0026rsquo;s capabilities in delivering fast and efficient LLM inference.\nImmediate Comments on the News From the authors\u0026rsquo; point of view, the new Rubin architecture is basically adding a KV cache layer to the existing GPU platform. The ideas are similar to the previous post about LMCache, but NVidia have also added some additional hardware components such as the DPUs (Data Processing Unit).\nOn the Groq side, this acqui-hire may be a defensive move for NVidia to reduce competitors. But regardless, ultra-low latency inference is crucial for real-time agents. Groq\u0026rsquo;s architecture is well-suited for this purpose and potentially can be integrated into future NVIDIA platforms.\nThese movements demonstrate NVidia\u0026rsquo;s investments into inference, which it has been talking about since 2025. This aligns with the problem faced by NVidia\u0026rsquo;s biggest end customer: Frontier Labs including OpenAI, Anthropic, \u0026hellip; One of the main doubts for these frontier labs before they raise more capital is their profitability. While people are racing for the best capability on benchmarks and providing training services like Tinker, so far inference is the only cash cow for them. Gross margin of Anthropic and OpenAI are rumored to be around 40% according to The Information, leaving plenty of room for improvement.\nImportance of KV Cache Hits In our previous blog post, we disussed the significance of KV cache hits in enhancing the efficiency of LLM agents. By storing previously computed KV Cache, agents can avoid redundant computations, leading to faster response times and reduced computational load.\nThe danger of a cache non-hit for any agent LLM trace is that it has to do the prefill for all the previous tokens again. For example, if an agent misses the previous KV cache for a 100K token context, it has to redo the prefill. On xx GPU with balba, prefill takes xx seconds. Since prefill is not batched, this will also block other tasks on the same GPU from being scheduled. Given that each agent can easily go over 20 turns, a few times of KV cache re-prefill can easily inflate the total latency by multiple times.\nHowever, if we are able to save and reuse the KV cache for long contexts, we can significantly reduce the full prefill time. For each round of the agents, we only need to do incremental prefill for the newly added user and agent messages. This can lead to substantial speedups by reduced computation.\nStory of Decoding Speed and the Memory Bottleneck However, we did not go into full depth of decoding in agent inference.\nAgents on many categories often have . We give a brief breakdown of the time spent in prefill and decoding for a SWE-agent task assuming this is the only task on the set of GPUs. Although decoding is often executed in batched manner to increase total throughput of the system, the latency experienced by each individual user will still be very long if there are many decoding tokens.\nBelow is a breakdown of time spent in prefill and decoding for a SWE-agent task assuming this is the only task on the set of GPUs.\nThe key bottleneck for decoding speed is the memory access pattern. During decoding, the model needs to frequently load the model weights as well as KV cache. This results in a minimal delay of total_memory / HBM bandwidth for each generated token. Below is a graph on the roofline model for GPU computations. Decoding falls into the memory bound regime, meaning that the execution time is dominated by memory access rather than how fast you can do the computations like matrix multiplication.\nAdmittedly, speculative decoding as discussed in this blog can help reduce the number of memory accesses by generating multiple tokens in one forward pass. However, the fundamental bottleneck remains: each forward pass still requires loading weights and KV cache from memory.\nGroq\u0026rsquo;s LPU architecture, which utilizes SRAM for storing model weights, offers a potential solution to this bottleneck. SRAM provides significantly faster access times compared to traditional memory types like HBM or DDR by directly putting memory on chip. This allows them to have 80TB/s bandwidth for SRAM as discussed by the blog. This allows them to have much faster vanilla (without speculative decoding) speed compared to traditional GPU architectures.\nWhy NVidia has the Potential to be the First to achieve real-time agents Let\u0026rsquo;s now assume we are the CTO of NVidia. Given the previous calculations, you will suddenly realize that if we just put the two pieces together: ICMS for KV cache hits and Groq\u0026rsquo;s SRAM-based architecture for decoding speed, we can potentially achieve real-time agents. Here is a rough sketch of the proposed architecture:\nOverall, the architecture we are proposing two components:\nPrefill node, which uses the compute optimized GPUs as compute units for incremental prefill and connects to many storage devices to store KV cache for long contexts. Decoding node, which uses specialized hardware that opimizes for memory access to achieve fast decoding. Below, we will do a simulation demonstrating how this architecture can potentially speed up agent inference from over minutes (now) to real-time (future).\nChallenges for Realizing the Proposed Architecture We outline some of the immediate challenges that come to authors\u0026rsquo; minds for realizing the proposed architecture:\nThe integration between NVidia\u0026rsquo;s GPU platform and Groq\u0026rsquo;s LPU architecture. This includes architecture design, data transfer protocols, and other compatibility issues. Author does not work on hardware design and thus cannot comment too much on the specific hardware. But it remains unclear how we can expose software APIs to allow seamless data transfer between the two hardwares.\nThe software stack for coordinating prefill and decoding nodes. Although the idea of ICMS is promising, software stack for efficiently coordinating between prefill and decoding nodes is non-trivial. This includes design of different caching policies, scheduling systems for balancing delay and throughput under agentic scenarios, and other system-level optimizations. There have been initial effort on software stack for LLM inference such as vLLM, SGLang, LMCache\u0026hellip; But customizing them for these specialized hardware workloads will remain challenging. Moreover, utilizing speculative decoding in the new architecture will also require significant software engineering efforts.\nConclusion The convergence of NVidia\u0026rsquo;s massive compute scale with Groq\u0026rsquo;s specialized decoding speed brings many future opportunites. By combining the newly released inference context management storage system for reusing KV cache and groq\u0026rsquo;s SRAM for rapid decoding, NVidia potentially can overcome the latency bottlenecks that currently plague complex agentic workflows. While our proposed architecture is theoretical, the strategic moves at CES 2026 suggest this is the direction the industry is heading. Real-time, context-aware agents are no longer just a software ambition; the hardware foundation is finally being laid to make them a reality.\n","permalink":"http://localhost:1313/blog/posts/fast_agent/","summary":"\u003cp\u003eIn this blog post, we will discuss NVidia\u0026rsquo;s recent announcements at CES and their acquisition of Groq, focusing on their strategy to enhance LLM agent inference.\nWe will explore three main aspects: the importance of KV cache hits, the role of SRAM in improving decoding speed, and a proposed hardware-software architecture that potentially speeds up agent inference from task-based to real-time.\u003c/p\u003e\n\u003cp\u003ePrerequisite: LLM inference basics.\nSuggested reading:\n\u003ca href=\"https://arpitbhayani.me/blogs/how-llm-inference-works/\"\u003eLLM Inference\u003c/a\u003e;   \u003ca href=\"https://blog.lmcache.ai/en/2024/09/17/lmcache-turboboosting-vllm-with-7x-faster-access-to-100x-more-kv-caches/\"\u003eKV Cache Offloading with LMCache\u003c/a\u003e; \u003ca href=\"https://hanchenli.github.io/blog/posts/kv_agent/\"\u003eLLM Agent with KV Cache\u003c/a\u003e\u003c/p\u003e","title":"CES and Groq Acqui-hire Reflection: NVidia's Potential to Build Real Time Agents"},{"content":"Agents are EXPENSIVE. Claude Code takes 1 USD to handle a single issue in a mid-sized public repository when using API. In the same time, it charges 20 USD per month for a subscription license. Why do we still get to use them? Maybe the price war, maybe the crazy debts some companies upstream are carrying, maybe the implicit labeling you are carrying out for the providers.\nBut regardless of the reason behind the pricing, we want to use more these powerful agents. Current models and applications are already capable of handling many complex tasks with minimal human intervention. However, the efficiency of these agents has just rised to our attention. In this blog post, we will discuss why agents are efficiency nightmares, how we can make them (somewhat) better with KV cache management tool like LMCache, and where we could be moving forward for improving agents.\nPrerequisite: LLM inference basics. Suggested reading: LLM Inference; KV Cache Offloading with LMCache\nPowerful Agents; But Draining Bank Account. Everyone said 2025 was the year of agents. With continuous improment in base model and emerging techniques like reinforcement learning, coding agents including Cursor, Claude Code have become much more powerful and automated compared with the start of the year. Quantitatively, the score on SWE-Bench has risen from 20% as in Aug 2024 to over 70% for frontier models with a simple agent scaffold.\nHowever, the cost of running these agents is still prohibitively high. We conducted a simple benchmark experiment running Claude Code with APIs to get a quantitative measure of the actual token cost. Our experiment with 30 random verified tasks from SWE-bench revealed some eye-opening numbers. The agent achieved a 63.3% resolution rate, successfully fixing 19 out of 30 complex repository issues autonomously. However, this performance comes with a cost: while the average expense was $0.74 per issue, a single complex Django bug could spike the cost to over $3.20. Most interestingly, over 90% of the token volume was spent on \u0026lsquo;cache reads,\u0026rsquo; indicating that the agent spent the vast majority of its budget repeatedly re-scanning the codebase to verify its steps.\nWe found that handling a single issue in a mid-sized public repository costs around 1 USD when using Claude Code via API. This is in stark contrast to the subscription license cost of 20 USD per month. There could be various reasons behind this pricing discrepancy, such as market dynamics, provider strategies, or the implicit value of the labeling work being done by the users. However, despite the high costs, the capabilities of these agents make them indispensable for many users. As proved by the usage-based pricing model adopted by Manus, many users are willing to pay for the advanced functionalities provided by these agents, even at a premium.\nHowever, as the adoption of these agents grows, the efficiency of their operation becomes increasingly important. This directly improves the Return-On-Investment (ROI) for users and makes these tools more accessible to a broader audience.\nThe Fundamental Inefficiency of Agents In order to understand why agents are so expensive to run, we need to look at how they operate fundamentally. We analyze trace from Claude Code using Claude-4.5-sonnet and mini-swe-agent using GPT-5 to give a brief overview. We used each agent to solve a coding task from SWE-bench_Verified dataset and recorded the message history sent. The cost report is based on token usage from the API responses and actual bills.\nOverview Statistics We report some high-level statistics from the traces below:\nMethod Average Total Tokens Average Cost ($) Average Calls to LLM Prefix Cache Hit Rate Claude Code 1,765,565.63 0.74 74.17 96.3% Mini-SWE-Agent 230,709.64 0.192 17.916 94.559% Microscopic View We randomly select one task (#80) from the SWE-bench_Verified dataset. The problem setup is to fix an issue in the django/django repo from commit 2e0f04507b17362239ba49830d26fec504d46978.\nProblem statement:\n\u0026ldquo;JSONField are not properly displayed in admin when they are readonly.\nDescription: JSONField values are displayed as dict when readonly in the admin. For example, {\u0026quot;foo\u0026quot;: \u0026quot;bar\u0026quot;} would be displayed as {'foo': 'bar'}, which is not valid JSON.\nI believe the fix would be to add a special case in django.contrib.admin.utils.display_for_field to call the prepare_value of the JSONField (not calling json.dumps directly to take care of the InvalidJSONInput case).\u0026rdquo;\nAnd this is exactly the prompt that Claude Code received.\nSurprisingly, before any fancy reasoning, Claude Code ran a couple of \u0026ldquo;warm-up\u0026rdquo; steps (trace ID #2, #3, #4) before the actual task. Warm-up steps do nothing but input the prompt for:\nTool list (#2) Explore subagent (#3) Plan subagent (#4) Warm-up steps are used for caching purposes—later when those tools and subagents are called, the cache will be hit, resulting in faster response time. The summarization agent (#1) and new topic agent (#5) are used for summarizing the context and generating a new title for display—just as the ChatGPT sidebar works.\nThe main agent (#6) comes with a huge system prompt, including git history, status, tool list, etc. The 18 tools in the tool list not only have the ability to use normal tool calls like Bash, Grep, Read, WebFetch, AskUserQuestion, etc., but also the ability to invoke and delegate certain tasks to subagents like:\nExplore subagent (#7) Plan subagent (#46) These subagents will invoke tool calls from their own tool lists.\nImmediately after the main agent (#6), it invokes the Explore (also called file search agent) subagent (#7), which will invoke tool calls from its tool list to explore the codebase. It starts with a different system prompt where its main goal is to explore the codebase:\nYou are Claude Code, Anthropic\u0026rsquo;s official CLI for Claude. You are a file search specialist for Claude Code, Anthropic\u0026rsquo;s official CLI for Claude. You excel at thoroughly navigating and exploring codebases.\nInterestingly, the Explore subagent (#7) is not the only subagent that Claude Code can invoke. Instead, it invokes 3 Explore subagents in parallel to explore the codebase, each with a different goal:\nExplore JSONField implementation (lifespan: #7-#26) Explore admin display_for_field (lifespan: #8-#37) Explore readonly field rendering (lifespan: #9-#45) The context of the main agent (#6) is not carried to the subagents, which is beneficial for the subagents to have a fresh start. Each Explore subagent can invoke 1-3 tools in parallel, where the tools are from the tool list of the Explore subagent—a subset (10/18) of the main agent\u0026rsquo;s tool list.\nThe ReAct mechanism is used here: the Explore subagent will invoke a tool call, then based on the tool output, it will observe and invoke another tool call to explore the codebase further until it deems it has explored enough.\nFinally, after the slowest Explore subagent finishes its exploration at step #45, at step #46, the main agent appends the findings (summarizations) from all 3 Explore subagents to the context, and then invokes the Plan subagent (#47) to plan the fix.\nSimilar to the Explore Agent, the Plan Agent (#47) also has a different system prompt, where its main goal is to plan the fix:\nYou are Claude Code, Anthropic\u0026rsquo;s official CLI for Claude. You are a software architect and planning specialist for Claude Code. Your role is to explore the codebase and design implementation plans.\nThe Plan Agent did not carry all the context from the main agent nor the Explore subagents, which is beneficial for the Plan Agent to have a fresh start. Instead, it only contains the summarization of the Explore subagents\u0026rsquo; findings. The toolbox is a subset (10/18) of the main agent\u0026rsquo;s tool list. The goal for the Plan Agent is to design an implementation plan that:\nPlease design an implementation plan that:\nIdentifies the exact changes needed to display_for_field Considers whether we need to instantiate a form field from the model field or if there\u0026rsquo;s a better approach Identifies any edge cases or potential issues Recommends the best approach given Django\u0026rsquo;s architecture Similarly, the Plan Agent also follows the ReAct pattern and loops through tool calling from #47 to #72, where the context accumulates from 11,552 tokens to 38,819 tokens. After having a good plan (see details in #72), the Plan Agent will return to the main agent (#73) with the plan.\nThe main agent will then invoke a series of tool calls to:\nReview the plan (#73) Ask user for clarification (#74) Write the plan into a markdown file (#75) Finally, the main agent will exit the plan mode (#76) and enter the execute mode (#77) to execute the plan after interactively asking the user for plan approval (#76-#77).\nThe execution phase (#77-#91) still follows the ReAct pattern. The main agent will use the plan markdown file as a todo list:\nAdd json import to utils.py Add JSONField handling to display_for_field() Add tests to test_admin_utils.py Run the tests to verify After executing some tool calls to read or edit files, it will cross out the todo items in the plan markdown file. Once all the todo items are crossed out, the main agent will end with a conclusion message (#92).\nDuring this phase, there are some other subagents being invoked—e.g., the Extract Bash Command subagent (#93), where there\u0026rsquo;s only a one-shot prompt template for the subagent to extract the bash command in order to not run dangerous commands like rm without user confirmation by accident.\nAnd this is the whole diagram of the claude code trace:\nKey Observations We highlight three key observations from the trace analysis above that contribute to the inefficiency of agents:\nHigh number of calls: Agents make a large number of calls to the LLM server due to the need to repetitive get actions and report status. Each call incurs incremental prefill and may cause KV cache miss that requires recomputation. Appending Context: Agents often append new context to the existing conversation history, leading to longer input sequences and increased computational load for each call. Fundamentally, due to the need to fully incorporate the context of the task into the LLM inputs and the heavy intermediate states (KV cache) related to these contexts, agents are highly memory-bound workloads. Sadly, memory bandwidth and capacity have not improved as fast as compute (FLOPS) in recent years.\nWhy is Current KV Cache Offloading Not Enough? From the table above, we can see that both agents have a high prefix cache hit rate (around 90%). Readers with background may wonder: 90+% Prefix Cache hit rate? Why not just run KV Cache Offloading and save 10x?\nHowever, there are three further considerations here:\nContention for Space: Due to the appending context behavior, the KV cache space is highly contended among different agent programs due to long context. Queueling Delay in Scheduling: While the current agent program is waiting on the tool, if the sched uler allocates the GPU memory to other requests to maximize throughput, the KV cache for the current program will be removed from GPU memory. This incurs a queueing delay for each request when the program resumes after tool execution. Remaining Compute: As mentioned above, each call still requires prefill compute even with KV cache hit. With a high number of calls, the remaining compute becomes non-negligible. These issue limits agents from fully benefitting from KV cache offloading. For example, in the graph above, we enabled vLLM with LMCache to serve agent traces under a certain jps. However, due to the high contention and queueling delay, each agent still incurs significant waiting time even with KV cache offloading enabled.\nSome research works have demonstrated by mitigating the first two issues, KV cache offloading can bring significant speedup and cost reduction for serving LLMs.\nFor example, Continuum proposes to reuse the Time-to-live (TTL) concept to preserve the KV cache for programs that are likely to be resumed soon to mitigate the first two problems. This changes the eviction policy from LRU to TTL-based, which is more suitable for agent workloads.\nAs demonstrated by the experiments, Continuum can bring up to 3.66x improvement for serving LLM agent traces with long context on SWE-Bench and Berkeley Function Calling Leaderboard workloads by improving upon naive KV cache offloading. The above graph shows the evaluations results. In the graph, each method represents a scheduling policy using vLLM and using KV cache offloading with LMCache. For A100 GPUs, we pair 100GB DRAM per GPU and for B200 GPUs, we pair 200GB DRAM per GPU. We set TP=4 for 70B model. The requests arrive according to a Poisson process with a certain JPS. The graphs show the average latency per request under different JPS.\nThe paper arxiv links is here: Continuum Paper, and a preview version of code is available at: Continuum Code.\nLooking Forward: Beyond KV Cache Management However, as shown in the graphs, each agent execution still takes significant time even with KV cache offloading. Even without any contention, each call still requires multiple long prefill and decode operations throughput the trace. These computations add up in the end.\nTo further improve the efficiency of agents, we need to look beyond just KV cache management. Some potential directions include:\nModel Optimization: Researchers have been proposing methods that reduces memory footprint for intermediate states recorded in the agent trace. Examples include KV cache compressions, and more efficient attention mechanisms like linear attention, Mamba mechanisms. Adaptive Context Management: As a recent trend, agentic context engineering (ACE) has been growing popularity. Many work proposes to filter the input text before feeding them into the LLM. Example works include ReSum, AgentFold, and Manus\u0026rsquo;s Context Engineering Report\u0026hellip; These methods prevents unncessary context from flooding the LLM context window, reducing computation from the source. Going forward, agents are and will probably remain memory-bound for the foreseeable future. Addressing this fundamental bottleneck requires a careful co-design of algorithms and systems. This enables not only powerful but efficient future agents. After all, ROI is the key metric for GenAI to succeed in the real industry in the long run.\nConclusion In this blog post, we discussed why these powerful agents are actually efficiency nightmares. The agents frequent sends LLM requests, have long context histories leading to large intermediate states, and incurs heavy use of GPU resources. Prefix caching provides a promising solution to prevent repetitive prefill but naive policies may not be sufficient. We demonstrated how KV cache sharing techniques like Continuum can help mitigate some of the inefficiencies. However, we also highlighted that further improvements are needed beyond KV cache management due to the memory-bound nature of long context inference, including model optimizations and adaptive context management strategies.\nAcknowledgements We would like to thank authors of Continuum and folks from LMCache and Tensormesh for their valuable discussions and feedback on this blog post.\nDisclaimer: This blog post was created with help from Gemini, VSCode copilot. Views are solely from the authors and do not reflect employer values.\n","permalink":"http://localhost:1313/blog/posts/kv_agent/","summary":"\u003cp\u003eAgents are EXPENSIVE. Claude Code takes 1 USD to handle a single issue in a mid-sized public repository when using API.\nIn the same time, it charges 20 USD per month for a subscription license.\nWhy do we still get to use them?\nMaybe the price war, maybe the crazy debts some companies upstream are carrying, maybe the implicit labeling you are carrying out for the providers.\u003c/p\u003e\n\u003cp\u003eBut regardless of the reason behind the pricing, we want to use more these powerful agents.\nCurrent models and applications are already capable of handling many complex tasks with minimal human intervention.\nHowever, the efficiency of these agents has just rised to our attention.\nIn this blog post, we will discuss why agents are efficiency nightmares, how we can make them (somewhat) better with KV cache management tool like LMCache,\nand where we could be moving forward for improving agents.\u003c/p\u003e","title":"Why Agents are Efficiency Nightmares and How to Fix them?"}]