<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Context Engineering &amp; Reuse Pattern Under the Hood of Claude Code | Hanchen&#39;s Space Bar</title>
<meta name="keywords" content="LLM, Agent, Claude Code, Context Engineering, Reuse Pattern">
<meta name="description" content="Recently, we ran a tiny one-shot experiment, and found that Claude Code has a high context reuse rate of 92%.">
<meta name="author" content="">
<link rel="canonical" href="https://hanchenli.github.io/blogs/posts/claude_code/">
<link crossorigin="anonymous" href="/blogs/assets/css/stylesheet.12d075fb361717557fefde57d93db59c2ce2143ebfcc84ea38467edf4e444ce7.css" integrity="sha256-EtB1&#43;zYXF1V/795X2T21nCziFD6/zITqOEZ&#43;305ETOc=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://hanchenli.github.io/blogs/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://hanchenli.github.io/blogs/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://hanchenli.github.io/blogs/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://hanchenli.github.io/blogs/apple-touch-icon.png">
<link rel="mask-icon" href="https://hanchenli.github.io/blogs/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://hanchenli.github.io/blogs/posts/claude_code/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<style>
     
    body:has(.post-single),
    body:has(.post-content) {
        background-image: url('/blogs/images/background.png');
        background-size: cover;
        background-repeat: no-repeat;
        background-attachment: fixed;
        background-position: center;
    }
</style>
 <meta property="og:url" content="https://hanchenli.github.io/blogs/posts/claude_code/">
  <meta property="og:site_name" content="Hanchen&#39;s Space Bar">
  <meta property="og:title" content="Context Engineering & Reuse Pattern Under the Hood of Claude Code">
  <meta property="og:description" content="Recently, we ran a tiny one-shot experiment, and found that Claude Code has a high context reuse rate of 92%.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-12-21T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-12-21T00:00:00+00:00">
    <meta property="article:tag" content="LLM">
    <meta property="article:tag" content="Agent">
    <meta property="article:tag" content="Claude Code">
    <meta property="article:tag" content="Context Engineering">
    <meta property="article:tag" content="Reuse Pattern">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Context Engineering &amp; Reuse Pattern Under the Hood of Claude Code">
<meta name="twitter:description" content="Recently, we ran a tiny one-shot experiment, and found that Claude Code has a high context reuse rate of 92%.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://hanchenli.github.io/blogs/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Context Engineering \u0026 Reuse Pattern Under the Hood of Claude Code",
      "item": "https://hanchenli.github.io/blogs/posts/claude_code/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Context Engineering \u0026 Reuse Pattern Under the Hood of Claude Code",
  "name": "Context Engineering \u0026 Reuse Pattern Under the Hood of Claude Code",
  "description": "Recently, we ran a tiny one-shot experiment, and found that Claude Code has a high context reuse rate of 92%.",
  "keywords": [
    "LLM", "Agent", "Claude Code", "Context Engineering", "Reuse Pattern"
  ],
  "articleBody": "Context Engineering \u0026 Reuse Pattern Under the Hood of Claude Code Over the last few months, Claude Code has quietly become one of the most interesting \u0026 widely-adopted real-world agentic systems available to normal developers.\nUnlike cloud-only agents whose internals remain hidden behind API gateways like Perplexity, Devin, or Manus, nor as fully open source agents like Mini SWE Agent or Terminus 2 where you can deploy locally with source code, Claude Code runs partially locally — it has a open-sourced client repo running on the local machine, which gives us a rare opportunity: to inject the traffic it sends and reverse engineering to see every single LLM call, every intermediate tool invocation, every tiny decision the agent makes.\nRecently, we ran a tiny one-shot experiment (one random task from the SWE-bench_Verified dataset) with Claude Code and captured everything into a raw log file with only LLM input\u0026output: claude_code_trace.jsonl. If you paste this trace into the visualizer, you can see the trace details.\nKey metrics:\n92 LLM calls (#1-#92) ~2M input tokens consumed 13 minutes total duration 92% prefix reuse rate The goal was simple:\nIf you give Claude Code one small task, what exactly happens behind the scenes?\nWhich LLM calls get made? In what order?\nWhere does context get reused? And how much of the prompt is stable prefix(seen) vs incremental content(new)?\nThis is our walk-through of that trace.\n1. What “Actually Happens” When Claude Code Runs a Simple Task Claude Code feels straightforward as a product — you type a request in your editor, it edits files or runs some bash commands. But under the hood, even a simple one-step request decomposes into a surprisingly structured internal loop.\nWe randomly select one task (#80) from the SWE-bench_Verified dataset. The problem setup is to fix an issue in the django/django repo from commit 2e0f04507b17362239ba49830d26fec504d46978.\nProblem statement:\n“JSONField are not properly displayed in admin when they are readonly.\nDescription: JSONField values are displayed as dict when readonly in the admin. For example, {\"foo\": \"bar\"} would be displayed as {'foo': 'bar'}, which is not valid JSON.\nI believe the fix would be to add a special case in django.contrib.admin.utils.display_for_field to call the prepare_value of the JSONField (not calling json.dumps directly to take care of the InvalidJSONInput case).”\nAnd this is exactly the prompt that Claude Code received.\nSurprisingly, before any fancy reasoning, Claude Code ran a couple of “warm-up” steps (trace ID #2, #3, #4) before the actual task. Warm-up steps do nothing but input the prompt for:\nTool list (#2) Explore subagent (#3) Plan subagent (#4) Warm-up steps are used for caching purposes—later when those tools and subagents are called, the cache will be hit, resulting in faster response time. The summarization agent (#1) and new topic agent (#5) are used for summarizing the context and generating a new title for display—just as the ChatGPT sidebar works.\nThe main agent (#6) comes with a huge system prompt, including git history, status, tool list, etc. The 18 tools in the tool list not only have the ability to use normal tool calls like Bash, Grep, Read, WebFetch, AskUserQuestion, etc., but also the ability to invoke and delegate certain tasks to subagents like:\nExplore subagent (#7) Plan subagent (#46) These subagents will invoke tool calls from their own tool lists.\nImmediately after the main agent (#6), it invokes the Explore (also called file search agent) subagent (#7), which will invoke tool calls from its tool list to explore the codebase. It starts with a different system prompt where its main goal is to explore the codebase:\nYou are Claude Code, Anthropic’s official CLI for Claude. You are a file search specialist for Claude Code, Anthropic’s official CLI for Claude. You excel at thoroughly navigating and exploring codebases.\nInterestingly, the Explore subagent (#7) is not the only subagent that Claude Code can invoke. Instead, it invokes 3 Explore subagents in parallel to explore the codebase, each with a different goal:\nExplore JSONField implementation (lifespan: #7-#26) Explore admin display_for_field (lifespan: #8-#37) Explore readonly field rendering (lifespan: #9-#45) The context of the main agent (#6) is not carried to the subagents, which is beneficial for the subagents to have a fresh start. Each Explore subagent can invoke 1-3 tools in parallel, where the tools are from the tool list of the Explore subagent—a subset (10/18) of the main agent’s tool list.\nThe ReAct mechanism is used here: the Explore subagent will invoke a tool call, then based on the tool output, it will observe and invoke another tool call to explore the codebase further until it deems it has explored enough.\nFinally, after the slowest Explore subagent finishes its exploration at step #45, at step #46, the main agent appends the findings (summarizations) from all 3 Explore subagents to the context, and then invokes the Plan subagent (#47) to plan the fix.\nSimilar to the Explore Agent, the Plan Agent (#47) also has a different system prompt, where its main goal is to plan the fix:\nYou are Claude Code, Anthropic’s official CLI for Claude. You are a software architect and planning specialist for Claude Code. Your role is to explore the codebase and design implementation plans.\nThe Plan Agent did not carry all the context from the main agent nor the Explore subagents, which is beneficial for the Plan Agent to have a fresh start. Instead, it only contains the summarization of the Explore subagents’ findings. The toolbox is a subset (10/18) of the main agent’s tool list. The goal for the Plan Agent is to design an implementation plan that:\nPlease design an implementation plan that:\nIdentifies the exact changes needed to display_for_field Considers whether we need to instantiate a form field from the model field or if there’s a better approach Identifies any edge cases or potential issues Recommends the best approach given Django’s architecture Similarly, the Plan Agent also follows the ReAct pattern and loops through tool calling from #47 to #72, where the context accumulates from 11,552 tokens to 38,819 tokens. After having a good plan (see details in #72), the Plan Agent will return to the main agent (#73) with the plan.\nThe main agent will then invoke a series of tool calls to:\nReview the plan (#73) Ask user for clarification (#74) Write the plan into a markdown file (#75) Finally, the main agent will exit the plan mode (#76) and enter the execute mode (#77) to execute the plan after interactively asking the user for plan approval (#76-#77).\nThe execution phase (#77-#91) still follows the ReAct pattern. The main agent will use the plan markdown file as a todo list:\nAdd json import to utils.py Add JSONField handling to display_for_field() Add tests to test_admin_utils.py Run the tests to verify After executing some tool calls to read or edit files, it will cross out the todo items in the plan markdown file. Once all the todo items are crossed out, the main agent will end with a conclusion message (#92).\nDuring this phase, there are some other subagents being invoked—e.g., the Extract Bash Command subagent (#93), where there’s only a one-shot prompt template for the subagent to extract the bash command in order to not run dangerous commands like rm without user confirmation by accident.\nAnd this is the whole diagram of the claude code trace:\n2. The Secret Pattern: Claude Code Is a Prefix Reuse Machine During our trace analysis, one phenomenon was so consistent it deserves its own section:\nClaude Code’s prompts are extremely prefix-heavy.\nPrefix reuse means that one part of the prompt prefix is seen in the previous prompts’ prefix. Across all phases, the prompt reuse rate is extremely high: 92%. For ReAct-based subagent loops, it’s even higher. If we run prefix-length analysis in particular sections:\nTrace ID Total Tokens Shared Prefix % Notes #1-#6 47,177 0.22% Warm-up and initial phase #7-#45 546,104 92.06% Explore subagent phase #47-#72 528,286 93.23% Plan subagent phase #73-#92 827,411 97.83% Main agent execution phase What does this mean? Claude Code’s architecture practically optimizes itself for KV cache reusage, even without explicitly trying.\n3. What is prefix caching and why should I care? At the heart of Large Language Model inference lies the KV cache (key-value cache) — a mechanism that stores intermediate attention computation results for previously processed tokens. During autoregressive generation, each new token needs to attend to all previous tokens, requiring expensive matrix multiplications. The KV cache stores the key and value matrices computed for earlier tokens, so they don’t need to be recomputed with each new token.\nPrefix caching leverages this by recognizing that when multiple requests share the same prompt prefix (like system instructions or document context), their KV cache computations are identical and can be reused across requests.\nMajor LLM providers have turned this into significant cost savings:\nOpenAI's Prompt Caching handles prefix caching automatically — it detects common prefixes longer than 1,024 tokens and caches them transparently, offering a 90% discount on cached input tokens (e.g., GPT-5.2 drops from $1.75 to $0.175 per million cached tokens) Anthropic's cache hit pricing gives developers explicit control over which prompt blocks to cache using special cache_control markers, charging a slightly higher cache write cost (1.25x base price for 5-minute cache, 2x for 1-hour cache) but delivering the same 90% discount on cache reads (Claude Sonnet 4.5: $0.30 per million tokens for cache reads versus $3.00 for base input), allowing fine-grained optimization for complex multi-turn conversations or document-heavy workflows To put this in perspective with Claude Code’s 92% prefix reuse pattern: processing 2M input tokens (our consumption for the experiment) without caching would cost $6.00 (2M × $3/MTok), but with prefix caching, the cost drops to just $1.152 (1.84M cache hits × $0.30/MTok + 0.16M cache writes × $3.75/MTok) — a savings of $4.85 (81% reduction) over one simple task.\nOpen-source inference engines have also embraced this paradigm:\nvLLM's automatic prefix caching transparently caches shared prefixes using its PagedAttention mechanism SGLang's RadixAttention employs a radix tree data structure to efficiently match and reuse the longest common prefixes across requests LMCache takes distributed KV caching even further by pooling cache storage across multiple nodes to maximize reuse at scale Beyond cost savings, prefix cache hits dramatically reduce TTFT (time to first token) — since the model can skip recomputing the entire prefix and only process the unique suffix, latency for subsequent requests with shared context can drop by 5-10x, making conversational agents and document-grounded applications far more responsive.\n4. What We Learned from This Tiny Trace Even though the task was trivial, the trace reveals a lot about Claude Code as a system:\nThe main system prompt is huge\nIt contains: Complete git repository state and history + full tool specifications (18 tools for main agent) + finally, execution phase instructions The prompt alone is 20,000+ tokens without conversation history Claude Code is built around specialized subagents\nSubagents receive only role-specific context, reducing bloat Separation of context allows the main agent to only run on the summarized subagent responses Parallel execution is used to maximize exploration efficiency\nSubagents are spawned in parallel with different search goals under their own ReAct loop This separation allows clean context and focused subtasks, distributing context evenly Tool calls are also run in parallel for the same benefits “Warm-up” calls prime the cache before real work begins\nThey load tool specifications into cache, prime subagent system prompts, and establish stable prefix baselines These calls drastically accelerate subsequent subagent invocations Claude works well with KV cache reuse\nClaude reaches up to 92% overall prefix reuse, perfect for KV cache reuse optimization Results in a significant cost savings of $4.85 (81% reduction) over one simple task Interactive planning improves transparency\nGives users control over what changes will be made Creates a natural breakpoint prompting the user for approval Responses allow the system to create a more refined executable todo list, improving workflow 5. Beyond Prefix Caching: Can We Do Better? Recently, there are some interesting research papers that try to improve non-prefix caching efficiency, such as CacheBlend, where optimizations can be made even on non-prefix (substring) caching.\nIn our trace, we can see that the subagents have a tool list that is a subset of the main agent’s tool list, which means that the subagents can reuse the main agent’s tool list descriptions. This is a good example of how to improve non-prefix caching efficiency.\nAnother scenario in our trace is that if the same file was read multiple times, the file content can be cached and reused, even though the file content is not a prefix. This can be extremely helpful when the file content is large and the file is read multiple times.\n",
  "wordCount" : "2112",
  "inLanguage": "en",
  "datePublished": "2025-12-21T00:00:00Z",
  "dateModified": "2025-12-21T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://hanchenli.github.io/blogs/posts/claude_code/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Hanchen's Space Bar",
    "logo": {
      "@type": "ImageObject",
      "url": "https://hanchenli.github.io/blogs/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://hanchenli.github.io/blogs/" accesskey="h" title="Hanchen&#39;s Space Bar (Alt + H)">
                    <img src="https://hanchenli.github.io/blogs/images/logo.png" alt="" aria-label="logo"
                        height="70">Hanchen&#39;s Space Bar</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>

<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Context Engineering &amp; Reuse Pattern Under the Hood of Claude Code
    </h1>
    <div class="post-description">
      Recently, we ran a tiny one-shot experiment, and found that Claude Code has a high context reuse rate of 92%.
    </div>
    <div class="post-meta"><span title='2025-12-21 00:00:00 +0000 UTC'>December 21, 2025</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#context-engineering--reuse-pattern-under-the-hood-of-claude-code" aria-label="Context Engineering &amp; Reuse Pattern Under the Hood of Claude Code">Context Engineering &amp; Reuse Pattern Under the Hood of Claude Code</a></li>
                <li>
                    <a href="#1-what-actually-happens-when-claude-code-runs-a-simple-task" aria-label="1. What “Actually Happens” When Claude Code Runs a Simple Task">1. What “Actually Happens” When Claude Code Runs a Simple Task</a></li>
                <li>
                    <a href="#2-the-secret-pattern-claude-code-is-a-prefix-reuse-machine" aria-label="2. The Secret Pattern: Claude Code Is a Prefix Reuse Machine">2. The Secret Pattern: Claude Code Is a Prefix Reuse Machine</a></li>
                <li>
                    <a href="#3-what-is-prefix-caching-and-why-should-i-care" aria-label="3. What is prefix caching and why should I care?">3. What is prefix caching and why should I care?</a></li>
                <li>
                    <a href="#4-what-we-learned-from-this-tiny-trace" aria-label="4. What We Learned from This Tiny Trace">4. What We Learned from This Tiny Trace</a></li>
                <li>
                    <a href="#5-beyond-prefix-caching-can-we-do-better" aria-label="5. Beyond Prefix Caching: Can We Do Better?">5. Beyond Prefix Caching: Can We Do Better?</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="context-engineering--reuse-pattern-under-the-hood-of-claude-code">Context Engineering &amp; Reuse Pattern Under the Hood of Claude Code<a hidden class="anchor" aria-hidden="true" href="#context-engineering--reuse-pattern-under-the-hood-of-claude-code">#</a></h1>
<p><img alt="Claude Code Architecture" loading="lazy" src="https://raw.githubusercontent.com/kobe0938/blog/master/claude-code/assets/claude_code_diagram.png"></p>
<p>Over the last few months, <a href="https://www.claude.com/product/claude-code">Claude Code</a> has quietly become one of the most interesting &amp; widely-adopted real-world agentic systems available to normal developers.</p>
<p>Unlike <em><strong>cloud-only agents</strong></em> whose internals remain hidden behind API gateways like <a href="https://www.perplexity.ai/api-platform">Perplexity</a>, <a href="https://devin.ai/">Devin</a>, or <a href="https://manus.im/">Manus</a>, nor as fully <em><strong>open source agents</strong></em> like <a href="https://github.com/SWE-agent/mini-swe-agent">Mini SWE Agent</a> or <a href="https://github.com/laude-institute/harbor/blob/main/src/harbor/agents/terminus_2/terminus_2.py">Terminus 2</a> where you can deploy locally with source code, Claude Code runs <em><strong>partially locally</strong></em> — it has a open-sourced <a href="https://github.com/anthropics/claude-code">client repo</a> running on the local machine, which gives us a rare opportunity: to inject the traffic it sends and reverse engineering <strong>to see every single LLM call</strong>, every intermediate <strong>tool invocation</strong>, every tiny decision the agent makes.</p>
<p>Recently, we ran a tiny one-shot experiment (one random task from the <a href="https://huggingface.co/datasets/princeton-nlp/SWE-bench_Verified">SWE-bench_Verified</a> dataset) with Claude Code and captured everything into a <strong>raw</strong> log file with only LLM input&amp;output: <a href="https://github.com/kobe0938/blog/blob/master/claude-code/claude_code_trace.jsonl"><strong><code>claude_code_trace.jsonl</code></strong></a>. If you paste <a href="https://github.com/kobe0938/blog/blob/master/claude-code/claude_code_trace.jsonl">this trace</a> into the <a href="https://v0-llm-agent-dashboard.vercel.app/">visualizer</a>, you can see the trace details.</p>
<p><strong>Key metrics:</strong></p>
<ul>
<li><strong>92 LLM calls</strong> (<code>#1-#92</code>)</li>
<li><strong>~2M input tokens</strong> consumed</li>
<li><strong>13 minutes</strong> total duration</li>
<li><strong>92% prefix reuse rate</strong></li>
</ul>
<p><img alt="Visualizer Screenshot" loading="lazy" src="https://raw.githubusercontent.com/kobe0938/blog/master/claude-code/assets/visualizer-screenshot.png"></p>
<p>The goal was simple:</p>
<blockquote>
<p>If you give Claude Code one small task, what exactly happens behind the scenes?</p>
<p>Which LLM calls get made? In what order?</p>
<p>Where does context get reused? And how much of the prompt is stable prefix(seen) vs incremental content(new)?</p>
</blockquote>
<p>This is our walk-through of that trace.</p>
<hr>
<h1 id="1-what-actually-happens-when-claude-code-runs-a-simple-task"><strong>1. What “Actually Happens” When Claude Code Runs a Simple Task</strong><a hidden class="anchor" aria-hidden="true" href="#1-what-actually-happens-when-claude-code-runs-a-simple-task">#</a></h1>
<p>Claude Code feels straightforward as a product — you type a request in your editor, it edits files or runs some bash commands. But under the hood, even a simple one-step request decomposes into a surprisingly structured internal loop.</p>
<p>We randomly select one <a href="https://huggingface.co/datasets/princeton-nlp/SWE-bench_Verified/viewer/default/test?views%5B%5D=test&amp;row=80">task (#80)</a> from the <a href="https://huggingface.co/datasets/princeton-nlp/SWE-bench_Verified">SWE-bench_Verified</a> dataset. The problem setup is to fix an issue in the <code>django/django</code> repo from commit <code>2e0f04507b17362239ba49830d26fec504d46978</code>.</p>
<p><strong>Problem statement:</strong></p>
<blockquote>
<p><em>&ldquo;JSONField are not properly displayed in admin when they are readonly.</em></p>
<p><em>Description: JSONField values are displayed as dict when readonly in the admin. For example, <code>{&quot;foo&quot;: &quot;bar&quot;}</code> would be displayed as <code>{'foo': 'bar'}</code>, which is not valid JSON.</em></p>
<p><em>I believe the fix would be to add a special case in <code>django.contrib.admin.utils.display_for_field</code> to call the <code>prepare_value</code> of the JSONField (not calling <code>json.dumps</code> directly to take care of the <code>InvalidJSONInput</code> case).&rdquo;</em></p>
</blockquote>
<p>And this is exactly the prompt that Claude Code received.</p>
<p><img alt="Trace 1-46" loading="lazy" src="https://raw.githubusercontent.com/kobe0938/blog/master/claude-code/assets/trace1-46.png"></p>
<p>Surprisingly, before any fancy reasoning, Claude Code ran a couple of <strong>&ldquo;warm-up&rdquo; steps</strong> (trace ID <code>#2</code>, <code>#3</code>, <code>#4</code>) before the actual task. Warm-up steps do nothing but input the prompt for:</p>
<ul>
<li>Tool list (<code>#2</code>)</li>
<li>Explore subagent (<code>#3</code>)</li>
<li>Plan subagent (<code>#4</code>)</li>
</ul>
<p>Warm-up steps are used for caching purposes—later when those tools and subagents are called, the cache will be hit, resulting in faster response time. The summarization agent (<code>#1</code>) and new topic agent (<code>#5</code>) are used for summarizing the context and generating a new title for display—just as the ChatGPT sidebar works.</p>
<p>The main agent (<code>#6</code>) comes with a huge system prompt, including git history, status, tool list, etc. The <strong>18 tools</strong> in the tool list not only have the ability to use normal tool calls like <code>Bash</code>, <code>Grep</code>, <code>Read</code>, <code>WebFetch</code>, <code>AskUserQuestion</code>, etc., but also the ability to invoke and delegate certain tasks to subagents like:</p>
<ul>
<li>Explore subagent (<code>#7</code>)</li>
<li>Plan subagent (<code>#46</code>)</li>
</ul>
<p>These subagents will invoke tool calls from their own tool lists.</p>
<p>Immediately after the main agent (<code>#6</code>), it invokes the <strong>Explore</strong> (also called file search agent) subagent (<code>#7</code>), which will invoke tool calls from its tool list to explore the codebase. It starts with a different system prompt where its main goal is to explore the codebase:</p>
<blockquote>
<p>You are Claude Code, Anthropic&rsquo;s official CLI for Claude. You are a file search specialist for Claude Code, Anthropic&rsquo;s official CLI for Claude. You excel at thoroughly navigating and exploring codebases.</p>
</blockquote>
<p>Interestingly, the Explore subagent (<code>#7</code>) is not the only subagent that Claude Code can invoke. Instead, it invokes <strong>3 Explore subagents in parallel</strong> to explore the codebase, each with a different goal:</p>
<ol>
<li><strong>Explore JSONField implementation</strong> (lifespan: <code>#7-#26</code>)</li>
<li><strong>Explore admin display_for_field</strong> (lifespan: <code>#8-#37</code>)</li>
<li><strong>Explore readonly field rendering</strong> (lifespan: <code>#9-#45</code>)</li>
</ol>
<p>The context of the main agent (<code>#6</code>) is <strong>not</strong> carried to the subagents, which is beneficial for the subagents to have a fresh start. Each Explore subagent can invoke <strong>1-3 tools in parallel</strong>, where the tools are from the tool list of the Explore subagent—a subset (<strong>10/18</strong>) of the main agent&rsquo;s tool list.</p>
<p>The <a href="https://arxiv.org/pdf/2210.03629">ReAct</a> mechanism is used here: the Explore subagent will invoke a tool call, then based on the tool output, it will observe and invoke another tool call to explore the codebase further until it deems it has explored enough.</p>
<p>Finally, after the slowest Explore subagent finishes its exploration at step <code>#45</code>, at step <code>#46</code>, the main agent appends the findings (summarizations) from all 3 Explore subagents to the context, and then invokes the Plan subagent (<code>#47</code>) to plan the fix.</p>
<hr>
<p><img alt="Trace 47-76" loading="lazy" src="https://raw.githubusercontent.com/kobe0938/blog/master/claude-code/assets/trace47-76.png"></p>
<p>Similar to the Explore Agent, the Plan Agent (<code>#47</code>) also has a different system prompt, where its main goal is to plan the fix:</p>
<blockquote>
<p>You are Claude Code, Anthropic&rsquo;s official CLI for Claude. You are a software architect and planning specialist for Claude Code. Your role is to explore the codebase and design implementation plans.</p>
</blockquote>
<p>The Plan Agent did not carry all the context from the main agent nor the Explore subagents, which is beneficial for the Plan Agent to have a fresh start. Instead, it only contains the <strong>summarization</strong> of the Explore subagents&rsquo; findings. The toolbox is a subset (<strong>10/18</strong>) of the main agent&rsquo;s tool list. The goal for the Plan Agent is to design an implementation plan that:</p>
<blockquote>
<p>Please design an implementation plan that:</p>
<ol>
<li>Identifies the exact changes needed to display_for_field</li>
<li>Considers whether we need to instantiate a form field from the model field or if there&rsquo;s a better approach</li>
<li>Identifies any edge cases or potential issues</li>
<li>Recommends the best approach given Django&rsquo;s architecture</li>
</ol>
</blockquote>
<hr>
<p><img alt="Trace 77-92" loading="lazy" src="https://raw.githubusercontent.com/kobe0938/blog/master/claude-code/assets/trace77-92.png"></p>
<p>Similarly, the Plan Agent also follows the ReAct pattern and loops through tool calling from <code>#47</code> to <code>#72</code>, where the context accumulates from <strong>11,552 tokens</strong> to <strong>38,819 tokens</strong>. After having a good plan (see details in <code>#72</code>), the Plan Agent will return to the main agent (<code>#73</code>) with the plan.</p>
<p>The main agent will then invoke a series of tool calls to:</p>
<ul>
<li>Review the plan (<code>#73</code>)</li>
<li>Ask user for clarification (<code>#74</code>)</li>
<li>Write the plan into a markdown file (<code>#75</code>)</li>
</ul>
<p>Finally, the main agent will exit the plan mode (<code>#76</code>) and enter the execute mode (<code>#77</code>) to execute the plan after interactively asking the user for plan approval (<code>#76-#77</code>).</p>
<p>The <strong>execution phase</strong> (<code>#77-#91</code>) still follows the ReAct pattern. The main agent will use the plan markdown file as a todo list:</p>
<blockquote>
<ol>
<li>Add json import to <code>utils.py</code></li>
<li>Add JSONField handling to <code>display_for_field()</code></li>
<li>Add tests to <code>test_admin_utils.py</code></li>
<li>Run the tests to verify</li>
</ol>
</blockquote>
<p>After executing some tool calls to read or edit files, it will cross out the todo items in the plan markdown file. Once all the todo items are crossed out, the main agent will end with a conclusion message (<code>#92</code>).</p>
<p>During this phase, there are some other subagents being invoked—e.g., the <strong>Extract Bash Command</strong> subagent (<code>#93</code>), where there&rsquo;s only a one-shot prompt template for the subagent to extract the bash command in order to not run dangerous commands like <code>rm</code> without user confirmation by accident.</p>
<p>And this is the whole diagram of the claude code trace:</p>
<p><img alt="Claude Code Trace Diagram" loading="lazy" src="https://raw.githubusercontent.com/kobe0938/blog/master/claude-code/assets/claude_code_diagram.png"></p>
<hr>
<h1 id="2-the-secret-pattern-claude-code-is-a-prefix-reuse-machine"><strong>2. The Secret Pattern: Claude Code Is a Prefix Reuse Machine</strong><a hidden class="anchor" aria-hidden="true" href="#2-the-secret-pattern-claude-code-is-a-prefix-reuse-machine">#</a></h1>
<p>During our trace analysis, one phenomenon was so consistent it deserves its own section:</p>
<blockquote>
<p><strong>Claude Code’s prompts are extremely prefix-heavy.</strong></p>
</blockquote>
<p>Prefix reuse means that one part of the prompt prefix is seen in the previous prompts&rsquo; prefix. Across all phases, the prompt reuse rate is extremely high: <strong>92%</strong>. For ReAct-based subagent loops, it&rsquo;s even higher. If we run prefix-length analysis in particular sections:</p>
<table>
  <thead>
      <tr>
          <th>Trace ID</th>
          <th>Total Tokens</th>
          <th>Shared Prefix %</th>
          <th>Notes</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><code>#1-#6</code></td>
          <td>47,177</td>
          <td>0.22%</td>
          <td>Warm-up and initial phase</td>
      </tr>
      <tr>
          <td><code>#7-#45</code></td>
          <td>546,104</td>
          <td>92.06%</td>
          <td>Explore subagent phase</td>
      </tr>
      <tr>
          <td><code>#47-#72</code></td>
          <td>528,286</td>
          <td>93.23%</td>
          <td>Plan subagent phase</td>
      </tr>
      <tr>
          <td><code>#73-#92</code></td>
          <td>827,411</td>
          <td>97.83%</td>
          <td>Main agent execution phase</td>
      </tr>
  </tbody>
</table>
<p>What does this mean? Claude Code’s architecture practically <strong>optimizes itself for KV cache reusage</strong>, even without explicitly trying.</p>
<hr>
<h1 id="3-what-is-prefix-caching-and-why-should-i-care"><strong>3. What is prefix caching and why should I care?</strong><a hidden class="anchor" aria-hidden="true" href="#3-what-is-prefix-caching-and-why-should-i-care">#</a></h1>
<p>At the heart of Large Language Model inference lies the <strong>KV cache</strong> (key-value cache) — a mechanism that stores intermediate attention computation results for previously processed tokens. During autoregressive generation, each new token needs to attend to all previous tokens, requiring expensive matrix multiplications. The KV cache stores the key and value matrices computed for earlier tokens, so they don&rsquo;t need to be recomputed with each new token.</p>
<p><strong>Prefix caching</strong> leverages this by recognizing that when multiple requests share the same prompt prefix (like system instructions or document context), their KV cache computations are identical and can be reused across requests.</p>
<p>Major LLM providers have turned this into significant cost savings:</p>
<ul>
<li><strong><a href="https://platform.openai.com/docs/guides/prompt-caching">OpenAI's Prompt Caching</a></strong> handles prefix caching <strong>automatically</strong> — it detects common prefixes longer than 1,024 tokens and caches them transparently, offering a <strong>90% discount</strong> on cached input tokens (e.g., GPT-5.2 drops from $1.75 to $0.175 per million cached tokens) <img alt="OpenAI Prompt Caching" loading="lazy" src="https://raw.githubusercontent.com/kobe0938/blog/master/claude-code/assets/openai_cache_pricing.png"></li>
<li><strong><a href="https://platform.claude.com/docs/en/build-with-claude/prompt-caching">Anthropic's cache hit pricing</a></strong> gives developers <strong>explicit control</strong> over which prompt blocks to cache using special <code>cache_control</code> markers, charging a slightly higher cache write cost (1.25x base price for 5-minute cache, 2x for 1-hour cache) but delivering the same <strong>90% discount</strong> on cache reads (Claude Sonnet 4.5: $0.30 per million tokens for cache reads versus $3.00 for base input), allowing fine-grained optimization for complex multi-turn conversations or document-heavy workflows <img alt="Anthropic Prompt Caching" loading="lazy" src="https://raw.githubusercontent.com/kobe0938/blog/master/claude-code/assets/anthropic_cache_pricing.png"></li>
</ul>
<p>To put this in perspective with Claude Code&rsquo;s 92% prefix reuse pattern: processing 2M input tokens (our consumption for the experiment) <strong>without caching</strong> would cost <strong>$6.00</strong> (2M × $3/MTok), but <strong>with prefix caching</strong>, the cost drops to just <strong>$1.152</strong> (1.84M cache hits × $0.30/MTok + 0.16M cache writes × $3.75/MTok) — a savings of <strong>$4.85 (81% reduction)</strong> over one simple task.</p>
<p>Open-source inference engines have also embraced this paradigm:</p>
<ul>
<li><strong><a href="https://docs.vllm.ai/en/latest/features/automatic_prefix_caching/">vLLM's automatic prefix caching</a></strong> transparently caches shared prefixes using its PagedAttention mechanism</li>
<li><strong><a href="https://docs.sglang.io/advanced_features/hicache_best_practices.html">SGLang's RadixAttention</a></strong> employs a radix tree data structure to efficiently match and reuse the longest common prefixes across requests</li>
<li><strong><a href="https://github.com/LMCache/lmcache">LMCache</a></strong> takes distributed KV caching even further by pooling cache storage across multiple nodes to maximize reuse at scale</li>
</ul>
<p>Beyond cost savings, prefix cache hits dramatically reduce <strong>TTFT (time to first token)</strong> — since the model can skip recomputing the entire prefix and only process the unique suffix, latency for subsequent requests with shared context can drop by 5-10x, making conversational agents and document-grounded applications far more responsive.</p>
<hr>
<h1 id="4-what-we-learned-from-this-tiny-trace"><strong>4. What We Learned from This Tiny Trace</strong><a hidden class="anchor" aria-hidden="true" href="#4-what-we-learned-from-this-tiny-trace">#</a></h1>
<p>Even though the task was trivial, the trace reveals a lot about Claude Code as a system:</p>
<p><strong>The main system prompt is huge</strong></p>
<ul>
<li>It contains: Complete git repository state and history + full tool specifications (18 tools for main agent) + finally, execution phase instructions</li>
<li>The prompt alone is <strong>20,000+ tokens</strong> without conversation history</li>
</ul>
<p><strong>Claude Code is built around specialized subagents</strong></p>
<ul>
<li>Subagents receive only role-specific context, reducing bloat</li>
<li>Separation of context allows the main agent to only run on the summarized subagent responses</li>
</ul>
<p><strong>Parallel execution is used to maximize exploration efficiency</strong></p>
<ul>
<li>Subagents are spawned in parallel with different search goals under their own ReAct loop</li>
<li>This separation allows clean context and focused subtasks, distributing context evenly</li>
<li>Tool calls are also run in parallel for the same benefits</li>
</ul>
<p><strong>&ldquo;Warm-up&rdquo; calls prime the cache before real work begins</strong></p>
<ul>
<li>They load tool specifications into cache, prime subagent system prompts, and establish stable prefix baselines</li>
<li>These calls drastically accelerate subsequent subagent invocations</li>
</ul>
<p><strong>Claude works well with KV cache reuse</strong></p>
<ul>
<li>Claude reaches up to <strong>92% overall prefix reuse</strong>, perfect for KV cache reuse optimization</li>
<li>Results in a significant cost savings of <strong>$4.85 (81% reduction)</strong> over one simple task</li>
</ul>
<p><strong>Interactive planning improves transparency</strong></p>
<ul>
<li>Gives users control over what changes will be made</li>
<li>Creates a natural breakpoint prompting the user for approval</li>
<li>Responses allow the system to create a more refined executable todo list, improving workflow</li>
</ul>
<hr>
<h1 id="5-beyond-prefix-caching-can-we-do-better"><strong>5. Beyond Prefix Caching: Can We Do Better?</strong><a hidden class="anchor" aria-hidden="true" href="#5-beyond-prefix-caching-can-we-do-better">#</a></h1>
<p>Recently, there are some interesting research papers that try to improve non-prefix caching efficiency, such as <a href="https://arxiv.org/abs/2405.16444">CacheBlend</a>, where optimizations can be made even on non-prefix (substring) caching.</p>
<p><img alt="CacheBlend" loading="lazy" src="https://raw.githubusercontent.com/kobe0938/blog/master/claude-code/assets/cache_blend.png"></p>
<p>In our trace, we can see that the subagents have a tool list that is a subset of the main agent&rsquo;s tool list, which means that the subagents can reuse the main agent&rsquo;s tool list descriptions. This is a good example of how to improve non-prefix caching efficiency.</p>
<p>Another scenario in our trace is that if the same file was read multiple times, the file content can be cached and reused, even though the file content is not a prefix. This can be extremely helpful when the file content is large and the file is read multiple times.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://hanchenli.github.io/blogs/tags/llm/">LLM</a></li>
      <li><a href="https://hanchenli.github.io/blogs/tags/agent/">Agent</a></li>
      <li><a href="https://hanchenli.github.io/blogs/tags/claude-code/">Claude Code</a></li>
      <li><a href="https://hanchenli.github.io/blogs/tags/context-engineering/">Context Engineering</a></li>
      <li><a href="https://hanchenli.github.io/blogs/tags/reuse-pattern/">Reuse Pattern</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="https://hanchenli.github.io/blogs/">Hanchen&#39;s Space Bar</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
