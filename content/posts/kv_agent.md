---
title: "LLM Agent Inference is repetitive"
date: 2025-11-11
description: "Blog Post Demonstrating the Benefits of KV Cache Sharing in Agent Serving"
tags: ["LLM", "serving", "Agent", "LMCache"]
categories: ["general"]
ShowToc: true
TocOpen: false
---

We will demonstrate some novel agent workflows and demonstrate how KV cache sharing can boost performance by ?x for these workflows.


## "Modern" LLM Agent Workflows

Welcome to your blog! This post demonstrates how to write content using Markdown.

## Landscape of LLM Inference



## Why the LLM inference were not designed for Agents?
<!-- talk about kv cache sharing problem -->

## How to make it better?
<!-- talk about how KV cache sharing can make it better -->

## Conclusion
<!-- We will talk more about improvement beyond KV Cache Offloading. -->